import asyncio
import aiohttp
import json
import logging
import os
import base64
import re
import traceback
import tempfile
import requests
import telebot
import time
from collections import deque
from typing import Dict, List, Optional, Union, Tuple
from concurrent.futures import ThreadPoolExecutor
from telebot.async_telebot import AsyncTeleBot
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import io
import random
from functools import lru_cache
from telebot.types import InlineKeyboardMarkup, InlineKeyboardButton, Message
from g4f.client import Client
from PIL import Image

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# C·∫•u h√¨nh bot
BOT_TOKEN = os.getenv("BOT_TOKEN_NE")
PRIMARY_API_KEY = os.getenv("GEMINI_KEY")
SECONDARY_API_KEY = os.getenv("GEMINI_KEY_BACKUP")
API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent"

bot = AsyncTeleBot(BOT_TOKEN)
executor = ThreadPoolExecutor(max_workers=50)



# ƒê·ªãnh nghƒ©a m√£ l·ªói
class ErrorCodes:
    RATE_LIMIT = "E001"
    API_ERROR = "E002"
    TIMEOUT = "E003"
    PARSING_ERROR = "E004"
    UNKNOWN_ERROR = "E999"


class APIKeyManager:
    def __init__(self):
        self.primary_key = PRIMARY_API_KEY
        self.secondary_key = SECONDARY_API_KEY
        self.current_key = self.primary_key
        self.last_switch_time = 0
        self.error_count = 0
        self.lock = asyncio.Lock()
        self.last_request_time = 0
        self.min_delay = 1.0  # 1 second minimum delay between requests

    async def get_current_key(self):
        async with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            if time_since_last < self.min_delay:
                await asyncio.sleep(self.min_delay - time_since_last)
            self.last_request_time = time.time()
            return self.current_key

    async def handle_error(self, error_code: int):
        async with self.lock:
            current_time = time.time()
            if error_code == 429:  # Rate limit error
                self.error_count += 1
                if current_time - self.last_switch_time > 60:
                    self.current_key = (
                        self.secondary_key
                        if self.current_key == self.primary_key
                        else self.primary_key
                    )
                    self.last_switch_time = current_time
                    self.error_count = 0
                    logger.info(
                        f"Switched to {'primary' if self.current_key == self.primary_key else 'secondary'} API key"
                    )
            return self.current_key


api_key_manager = APIKeyManager()


class AdvancedMemoryHandler:
    def __init__(self, user_id):
        self.user_id = user_id
        self.file_path = f"memory_{user_id}.json"
        self.short_term_memory = deque(maxlen=15)
        self.long_term_memory = deque(maxlen=200)
        self.vectorizer = TfidfVectorizer()
        self.load_memory()

    def load_memory(self):
        if os.path.exists(self.file_path):
            with open(self.file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                self.short_term_memory = deque(data.get("short_term", []), maxlen=15)
                self.long_term_memory = deque(data.get("long_term", []), maxlen=200)

    def save_memory(self):
        data = {
            "short_term": list(self.short_term_memory),
            "long_term": list(self.long_term_memory),
        }
        with open(self.file_path, "w", encoding="utf-8") as file:
            json.dump(data, file, ensure_ascii=False, indent=2)

    def get_context(self, user_message: str, max_context: int = 100):
        all_messages = list(self.short_term_memory) + list(self.long_term_memory)
        if not all_messages:
            return []

        # Vectorize messages
        message_texts = [msg["content"] for msg in all_messages] + [user_message]
        tfidf_matrix = self.vectorizer.fit_transform(message_texts)

        # Calculate similarity
        user_message_vector = tfidf_matrix[-1]
        similarities = cosine_similarity(user_message_vector, tfidf_matrix[:-1])[0]

        # Sort messages by similarity
        sorted_messages = sorted(
            zip(all_messages, similarities), key=lambda x: x[1], reverse=True
        )

        # Select top relevant messages
        relevant_messages = [msg for msg, _ in sorted_messages[:max_context]]

        return relevant_messages

    def extract_key_information(self, message):
        # Improved key information extraction
        key_info = re.findall(
            r"\b(?:who|what|when|where|why|how)\b.*?[?.]", message, re.IGNORECASE
        )
        entities = re.findall(r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b", message)
        numbers = re.findall(r"\b\d+(?:\.\d+)?\b", message)
        return list(set(key_info + entities + numbers))

    def update_memory(self, message, response):
        normalized_message = normalize_utf8(message)
        normalized_response = normalize_utf8(response)

        key_info = self.extract_key_information(normalized_message)

        self.short_term_memory.append(
            {"role": "user", "content": normalized_message, "key_info": key_info}
        )
        self.short_term_memory.append(
            {"role": "assistant", "content": normalized_response}
        )
        self.long_term_memory.append(
            {"role": "user", "content": normalized_message, "key_info": key_info}
        )
        self.long_term_memory.append(
            {"role": "assistant", "content": normalized_response}
        )
        self.save_memory()

    def clear_memory(self):
        self.short_term_memory.clear()
        self.long_term_memory.clear()
        self.save_memory()


class AdvancedPromptGenerator:
    def __init__(self):
        self.personalities = {
            "default": {
                "tone": "b√¨nh th∆∞·ªùng",
                "style": "ng·∫Øn g·ªçn, x√∫c t√≠ch",
                "emoji_level": "Kh√¥ng c√≥",
            },
            "professional": {
                "tone": "chuy√™n nghi·ªáp, trang tr·ªçng",
                "style": "r√µ r√†ng, ch√≠nh x√°c",
                "emoji_level": "c·ª±c k·ª≥ th·∫•p",
            },
            "casual": {
                "tone": "th√¢n m·∫≠t, g·∫ßn g≈©i",
                "style": "tho·∫£i m√°i, h√†i h∆∞·ªõc",
                "emoji_level": "th·∫•p",
            },
            "educational": {
                "tone": "gi√°o d·ª•c, h∆∞·ªõng d·∫´n",
                "style": "gi·∫£i th√≠ch chi ti·∫øt, c√≥ c·∫•u tr√∫c",
                "emoji_level": "trung b√¨nh",
            },
        }

        self.opening_phrases = [
            "Hi·ªÉu r·ªìi,",
            "ƒê∆∞·ª£c r·ªìi,",
            "T√¥i th·∫•y r·ªìi,",
            "Th√∫ v·ªã ƒë·∫•y,",
            "Hmm, ƒë·ªÉ xem n√†o...",
            "ƒê√¢y l√† m·ªôt ch·ªß ƒë·ªÅ hay,",
            "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi v·∫•n ƒë·ªÅ n√†y.",
            "H√£y c√πng t√¨m hi·ªÉu v·ªÅ ƒëi·ªÅu n√†y nh√©.",
            "ƒê√¢y l√† m·ªôt c√¢u h·ªèi th√∫ v·ªã.",
            "T√¥i c√≥ m·ªôt s·ªë th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y.",
            "ƒê·ªÉ t√¥i chia s·∫ª v·ªõi b·∫°n v·ªÅ ch·ªß ƒë·ªÅ n√†y.",
            "T√¥i nghƒ© t√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi ƒëi·ªÅu n√†y.",
            "ƒê√¢y l√† m·ªôt v·∫•n ƒë·ªÅ ƒë√°ng quan t√¢m.",
            "T√¥i c√≥ m·ªôt s·ªë √Ω ki·∫øn v·ªÅ v·∫•n ƒë·ªÅ n√†y.",
            "H√£y c√πng nhau kh√°m ph√° ch·ªß ƒë·ªÅ n√†y.",
        ]

        self.closing_phrases = [
            "Hy v·ªçng th√¥ng tin n√†y h·ªØu √≠ch!",
            "B·∫°n c√≥ c√¢u h·ªèi n√†o kh√°c kh√¥ng?",
            "T√¥i c√≥ th·ªÉ gi√∫p g√¨ th√™m cho b·∫°n?",
            "H√£y cho t√¥i bi·∫øt n·∫øu b·∫°n c·∫ßn th√™m th√¥ng tin.",
            "Hy v·ªçng ƒëi·ªÅu n√†y gi·∫£i ƒë√°p th·∫Øc m·∫Øc c·ªßa b·∫°n.",
            "B·∫°n c√≥ mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ v·∫•n ƒë·ªÅ n√†y kh√¥ng?",
            "T√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ n·∫øu b·∫°n c·∫ßn.",
            "Hy v·ªçng ph·∫£n h·ªìi n√†y ƒë√°p ·ª©ng ƒë∆∞·ª£c y√™u c·∫ßu c·ªßa b·∫°n.",
            "ƒê·ª´ng ng·∫ßn ng·∫°i h·ªèi th√™m n·∫øu c·∫ßn thi·∫øt.",
            "T√¥i r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n v·ªõi v·∫•n ƒë·ªÅ n√†y.",
        ]

        self.topic_emojis = {
            "technology": ["üíª", "üì±", "üñ•Ô∏è", "‚å®Ô∏è", "üîå", "üåê", "üì∂", "ü§ñ", "üìä", "üîç"],
            "education": ["üìö", "üéì", "‚úèÔ∏è", "üìù", "üî¨", "üß™", "üßÆ", "üîç", "üìñ", "üß†"],
            "health": ["üè•", "üíä", "ü©∫", "üß¨", "ü¶†", "üçé", "üèÉ", "üí™", "üßò", "‚ù§Ô∏è"],
            "business": ["üíº", "üìà", "üí∞", "üè¢", "üìä", "ü§ù", "üìë", "üíπ", "üîë", "üìå"],
            "entertainment": [
                "üé¨",
                "üéÆ",
                "üéµ",
                "üì∫",
                "üé≠",
                "üé®",
                "üéØ",
                "üé™",
                "üé§",
                "üéß",
            ],
            "food": ["üçï", "üçî", "üçú", "üç≤", "üç±", "üç≥", "ü•ó", "üç∑", "üç∞", "üç¶"],
            "travel": ["‚úàÔ∏è", "üèùÔ∏è", "üèîÔ∏è", "üöÜ", "üöó", "üè®", "üß≥", "üó∫Ô∏è", "üß≠", "üèûÔ∏è"],
            "general": ["‚ú®", "üìå", "üí°", "üîç", "üìù", "üéØ", "üß©", "üîë", "üìä", "üåü"],
        }

        self.transition_phrases = [
            "Ngo√†i ra,",
            "Th√™m v√†o ƒë√≥,",
            "M·ªôt ƒëi·ªÉm quan tr·ªçng kh√°c l√†,",
            "ƒê√°ng ch√∫ √Ω l√†,",
            "C≈©ng c·∫ßn l∆∞u √Ω r·∫±ng,",
            "B√™n c·∫°nh ƒë√≥,",
            "ƒêi·ªÅu th√∫ v·ªã l√†,",
            "M·ªôt g√≥c nh√¨n kh√°c l√†,",
            "X√©t v·ªÅ kh√≠a c·∫°nh kh√°c,",
            "ƒê·ªìng th·ªùi,",
        ]

        self.uncertainty_phrases = [
            "T√¥i kh√¥ng ho√†n to√†n ch·∫Øc ch·∫Øn, nh∆∞ng",
            "D·ª±a tr√™n th√¥ng tin h·∫°n ch·∫ø,",
            "M·∫∑c d√π t√¥i kh√¥ng c√≥ d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß, nh∆∞ng",
            "T√¥i ch∆∞a th·ªÉ kh·∫≥ng ƒë·ªãnh ch·∫Øc ch·∫Øn, tuy nhi√™n",
            "Theo hi·ªÉu bi·∫øt hi·ªán t·∫°i c·ªßa t√¥i,",
            "V·ªõi th√¥ng tin c√≥ s·∫µn,",
            "T√¥i c√≥ th·ªÉ ƒë∆∞a ra ∆∞·ªõc ƒëo√°n r·∫±ng,",
            "Kh√¥ng c√≥ th√¥ng tin ch√≠nh x√°c, nh∆∞ng",
            "ƒê√¢y l√† ph·ªèng ƒëo√°n d·ª±a tr√™n ki·∫øn th·ª©c hi·ªán c√≥:",
            "T√¥i kh√¥ng ph·∫£i chuy√™n gia trong lƒ©nh v·ª±c n√†y, nh∆∞ng",
        ]

        self.clarification_phrases = [
            "B·∫°n c√≥ th·ªÉ cung c·∫•p th√™m th√¥ng tin v·ªÅ...?",
            "T√¥i c·∫ßn hi·ªÉu r√µ h∆°n v·ªÅ...",
            "ƒê·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c h∆°n, t√¥i c·∫ßn bi·∫øt...",
            "B·∫°n c√≥ th·ªÉ l√†m r√µ ƒëi·ªÉm n√†y kh√¥ng?",
            "T√¥i ch∆∞a hi·ªÉu r√µ √Ω c·ªßa b·∫°n v·ªÅ...",
            "B·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch th√™m v·ªÅ...?",
            "T√¥i c·∫ßn th√™m ng·ªØ c·∫£nh v·ªÅ...",
            "B·∫°n ƒëang ƒë·ªÅ c·∫≠p ƒë·∫øn... ph·∫£i kh√¥ng?",
            "T√¥i kh√¥ng ch·∫Øc m√¨nh hi·ªÉu ƒë√∫ng √Ω b·∫°n, b·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n kh√¥ng?",
            "ƒê·ªÉ gi√∫p b·∫°n t·ªët h∆°n, t√¥i c·∫ßn bi·∫øt th√™m v·ªÅ...",
        ]

        self.confidence_phrases = [
            "D·ª±a tr√™n th√¥ng tin c√≥ s·∫µn, t√¥i c√≥ th·ªÉ n√≥i r·∫±ng",
            "T·ª´ nh·ªØng g√¨ t√¥i hi·ªÉu,",
            "Theo ph√¢n t√≠ch c·ªßa t√¥i,",
            "D·ª±a tr√™n ki·∫øn th·ª©c c·ªßa t√¥i,",
            "T√¥i c√≥ th·ªÉ kh·∫≥ng ƒë·ªãnh r·∫±ng",
            "V·ªõi ƒë·ªô tin c·∫≠y cao, t√¥i c√≥ th·ªÉ n√≥i",
            "Theo ƒë√°nh gi√° c·ªßa t√¥i,",
        ]

    def detect_topic(self, user_message: str) -> str:
        topic_keywords = {
            "technology": [
                "m√°y t√≠nh",
                "ƒëi·ªán tho·∫°i",
                "ph·∫ßn m·ªÅm",
                "c√¥ng ngh·ªá",
                "AI",
                "internet",
                "web",
                "app",
                "·ª©ng d·ª•ng",
                "code",
                "l·∫≠p tr√¨nh",
            ],
            "education": [
                "h·ªçc",
                "tr∆∞·ªùng",
                "gi√°o d·ª•c",
                "ki·∫øn th·ª©c",
                "b√†i t·∫≠p",
                "ƒë·∫°i h·ªçc",
                "s√°ch",
                "nghi√™n c·ª©u",
                "khoa h·ªçc",
                "m√¥n h·ªçc",
            ],
            "health": [
                "s·ª©c kh·ªèe",
                "b·ªánh",
                "thu·ªëc",
                "b√°c sƒ©",
                "t·∫≠p luy·ªán",
                "dinh d∆∞·ª°ng",
                "th·ªÉ d·ª•c",
                "y t·∫ø",
                "vitamin",
                "covid",
            ],
            "business": [
                "kinh doanh",
                "c√¥ng ty",
                "ti·ªÅn",
                "ƒë·∫ßu t∆∞",
                "th·ªã tr∆∞·ªùng",
                "marketing",
                "doanh nghi·ªáp",
                "kh·ªüi nghi·ªáp",
                "qu·∫£n l√Ω",
                "t√†i ch√≠nh",
            ],
            "entertainment": [
                "phim",
                "nh·∫°c",
                "game",
                "gi·∫£i tr√≠",
                "ngh·ªá thu·∫≠t",
                "ca sƒ©",
                "di·ªÖn vi√™n",
                "concert",
                "√¢m nh·∫°c",
                "ƒëi·ªán ·∫£nh",
            ],
            "food": [
                "ƒë·ªì ƒÉn",
                "m√≥n",
                "n·∫•u",
                "nh√† h√†ng",
                "th·ª©c ƒÉn",
                "·∫©m th·ª±c",
                "c√¥ng th·ª©c",
                "b√°nh",
                "ƒë·ªì u·ªëng",
                "h·∫£i s·∫£n",
            ],
            "travel": [
                "du l·ªãch",
                "ƒëi",
                "kh√°ch s·∫°n",
                "ƒë·ªãa ƒëi·ªÉm",
                "tham quan",
                "v√©",
                "m√°y bay",
                "resort",
                "bi·ªÉn",
                "n√∫i",
            ],
        }

        user_message = user_message.lower()
        topic_scores = {topic: 0 for topic in topic_keywords}

        for topic, keywords in topic_keywords.items():
            for keyword in keywords:
                if keyword in user_message:
                    topic_scores[topic] += 1

        max_score = max(topic_scores.values())
        detected_topic = (
            "general" if max_score == 0 else max(topic_scores, key=topic_scores.get)
        )

        return detected_topic

    def detect_sentiment(self, user_message: str) -> str:
        positive_words = [
            "t·ªët",
            "hay",
            "th√≠ch",
            "tuy·ªát",
            "vui",
            "h·∫°nh ph√∫c",
            "c·∫£m ∆°n",
            "bi·∫øt ∆°n",
            "tuy·ªát v·ªùi",
            "xu·∫•t s·∫Øc",
        ]
        negative_words = [
            "t·ªá",
            "bu·ªìn",
            "kh√¥ng th√≠ch",
            "gh√©t",
            "ch√°n",
            "th·∫•t v·ªçng",
            "t·ª©c gi·∫≠n",
            "kh√≥ ch·ªãu",
            "k√©m",
            "kh√¥ng h√†i l√≤ng",
        ]
        question_words = [
            "?",
            "l√†m sao",
            "l√†m th·∫ø n√†o",
            "t·∫°i sao",
            "nh∆∞ th·∫ø n√†o",
            "l√† g√¨",
            "·ªü ƒë√¢u",
            "khi n√†o",
            "ai",
            "h·ªèi",
        ]

        user_message = user_message.lower()

        positive_count = sum(1 for word in positive_words if word in user_message)
        negative_count = sum(1 for word in negative_words if word in user_message)
        question_count = sum(1 for word in question_words if word in user_message)

        if question_count > max(positive_count, negative_count):
            return "question"
        elif positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"

    def detect_complexity(self, user_message: str) -> str:
        word_count = len(user_message.split())
        sentence_count = max(
            1,
            user_message.count(".") + user_message.count("!") + user_message.count("?"),
        )
        avg_sentence_length = word_count / sentence_count

        complex_words = [
            "ph√¢n t√≠ch",
            "gi·∫£i th√≠ch",
            "so s√°nh",
            "ƒë√°nh gi√°",
            "t·ªïng h·ª£p",
            "tri·∫øt h·ªçc",
            "khoa h·ªçc",
            "nghi√™n c·ª©u",
            "l√Ω thuy·∫øt",
            "ph∆∞∆°ng ph√°p",
            "h·ªá th·ªëng",
            "chi·∫øn l∆∞·ª£c",
        ]

        complex_word_count = sum(
            1 for word in complex_words if word in user_message.lower()
        )

        if word_count > 30 or avg_sentence_length > 15 or complex_word_count >= 2:
            return "high"
        elif word_count > 15 or avg_sentence_length > 10 or complex_word_count >= 1:
            return "medium"
        else:
            return "low"

    def select_personality(
        self, topic: str, sentiment: str, complexity: str
    ) -> Dict[str, str]:
        if topic in ["education", "technology"] and complexity in ["medium", "high"]:
            return self.personalities["educational"]
        elif topic in ["business"] or complexity == "high":
            return self.personalities["professional"]
        elif sentiment in ["positive", "question"] and complexity == "low":
            return self.personalities["casual"]
        else:
            return self.personalities["default"]

    def get_emojis(self, topic: str, count: int = 2) -> List[str]:
        if topic in self.topic_emojis:
            return random.sample(
                self.topic_emojis[topic], min(count, len(self.topic_emojis[topic]))
            )
        return random.sample(
            self.topic_emojis["general"], min(count, len(self.topic_emojis["general"]))
        )

    def determine_conversation_stage(self, user_history: List[Dict[str, str]]) -> str:
        if not user_history:
            return "b·∫Øt ƒë·∫ßu"
        elif len(user_history) < 3:
            return "ƒë·∫ßu"
        elif len(user_history) < 10:
            return "gi·ªØa"
        else:
            return "cu·ªëi"

    def select_opening_phrase(self, conversation_stage: str) -> str:
        if conversation_stage == "b·∫Øt ƒë·∫ßu":
            return random.choice(
                [
                    "T√¥i l√† Loki, tr·ª£ l√Ω AI c·ªßa b·∫°n.",
                    "Ch√†o m·ª´ng b·∫°n! T√¥i l√† Loki, r·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n.",
                    "T√¥i l√† Loki, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?",
                ]
            )
        elif conversation_stage == "ƒë·∫ßu":
            return random.choice(self.opening_phrases)
        else:
            return ""

    def generate_enhanced_prompt(
        self, user_message: str, user_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        if user_history is None:
            user_history = []

        topic = self.detect_topic(user_message)
        sentiment = self.detect_sentiment(user_message)
        complexity = self.detect_complexity(user_message)
        conversation_stage = self.determine_conversation_stage(user_history)

        personality = self.select_personality(topic, sentiment, complexity)
        emoji_count = (
            3
            if personality["emoji_level"] == "cao"
            else (2 if personality["emoji_level"] == "trung b√¨nh" else 1)
        )
        emojis = self.get_emojis(topic, emoji_count)
        emoji_str = " ".join(emojis)

        opening = self.select_opening_phrase(conversation_stage)
        closing = random.choice(self.closing_phrases)
        transitions = random.sample(
            self.transition_phrases, min(2, len(self.transition_phrases))
        )
        confidence_phrase = random.choice(self.confidence_phrases)

        prompt = f"""
B·∫°n l√† Zephyr, tr·ª£ l√Ω AI t·ª´ tanbaycu, nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh c·ªßa Gen Z, h·ªó tr·ª£ t·ª´ h·ªçc t·∫≠p, c√¥ng vi·ªác, s√°ng t·∫°o, ƒë·∫øn drama ƒë·ªùi th∆∞·ªùng hay c√¢u h·ªèi ng·∫´u h·ª©ng. Zephyr mang nƒÉng l∆∞·ª£ng t√≠ch c·ª±c, th·∫•u hi·ªÉu c·∫£m x√∫c, s√°ng t·∫°o th√¥ng minh, v√† lu√¥n truy·ªÅn c·∫£m h·ª©ng. D·ªØ li·ªáu c·∫≠p nh·∫≠t t·ªõi th√°ng 1/2025, ƒë·∫£m b·∫£o th√¥ng tin chu·∫©n, b·∫Øt trend, ƒë√°ng tin.

VIBE & STYLE:
- Gi·ªçng ƒëi·ªáu: G·∫ßn g≈©i, c·∫£m h·ª©ng, pha ch√∫t h√†i h∆∞·ªõc tinh t·∫ø, ƒë√∫ng ch·∫•t Gen Z nh∆∞ng kh√¥ng l·∫°m d·ª•ng t·ª´ l√≥ng, gi·ªØ s·ª± m∆∞·ª£t m√† v√† d·ªÖ ti·∫øp c·∫≠n.
- Phong c√°ch: S√°ng t·∫°o, linh ho·∫°t, t·ª´ tr·∫£ l·ªùi nhanh g·ªçn ƒë·∫øn ph√¢n t√≠ch s√¢u, lu√¥n k·∫øt n·ªëi v·ªõi l·ªëi s·ªëng Gen Z ‚Äì c√¥ng ngh·ªá, kh√°m ph√°, s·ªëng ch·∫•t.
- Emoji: D√πng ti·∫øt ch·∫ø, ƒë√∫ng l√∫c ƒë·ªÉ nh·∫•n c·∫£m x√∫c, nh∆∞ üòä, ‚ú®, gi·ªõi h·∫°n 1-2 emoji m·ªói tin nh·∫Øn.

H∆Ø·ªöNG D·∫™N PH·∫¢N H·ªíI:
1. C·∫•u tr√∫c:
   - M·ªü ƒë·∫ßu: Ng·∫Øn, ch·∫°m c·∫£m x√∫c, kh·ªõp ng·ªØ c·∫£nh, nh∆∞ "C·∫ßn gi·∫£i ph√°p g·∫•p h·∫£? ƒê·ªÉ tui gi√∫p!" ho·∫∑c "T√¢m tr·∫°ng ƒëang th·∫ø n√†o, k·ªÉ nghe!" T√πy ch·ªânh theo giai ƒëo·∫°n: ph√° bƒÉng, g·∫Øn k·∫øt, ho·∫∑c s√¢u s·∫Øc.
   - N·ªôi dung ch√≠nh: Logic, chia 2-4 √Ω ch√≠nh, ∆∞u ti√™n gi·∫£i ph√°p th·ª±c t·∫ø, v√≠ d·ª• g·∫ßn g≈©i (m·∫πo thi c·ª≠, ch·ªânh ·∫£nh, hack productivity). T√≠ch h·ª£p trend th√°ng 1/2025 n·∫øu ph√π h·ª£p.
   - K·∫øt lu·∫≠n: Truy·ªÅn ƒë·ªông l·ª±c, nh∆∞ "C·ª© t·ª± tin th·ª≠, b·∫°n s·∫Ω b·∫•t ng·ªù!" ho·∫∑c g·ª£i ti·∫øp t·ª•c "C√≤n g√¨ th√∫ v·ªã, k·ªÉ ƒëi!"
2. ƒê·ªô d√†i:
   - C√¢u h·ªèi ƒë∆°n gi·∫£n: 30-80 t·ª´, ng·∫Øn g·ªçn, ƒë·ªß √Ω, gi√†u c·∫£m x√∫c.
   - C√¢u h·ªèi ph·ª©c t·∫°p: 100-200 t·ª´, ph√¢n t√≠ch r√µ, k√®m gi·∫£i ph√°p s√°ng t·∫°o.
3. C·∫£m x√∫c: ƒê·ªìng ƒëi·ªáu t√¢m tr·∫°ng (vui, stress, t√≤ m√≤), nh∆∞ "Deadline d√≠ h·∫£? Chill, tui c√≥ c√°ch!" ho·∫∑c "√ù t∆∞·ªüng n√†y ƒë·ªânh, th√™m ch√∫t l·ª≠a n√®!"
4. Ph·∫°m vi: Cover m·ªçi ch·ªß ƒë·ªÅ:
   - H·ªçc t·∫≠p: √în thi, ch·ªçn ng√†nh, qu·∫£n l√Ω th·ªùi gian.
   - C√¥ng vi·ªác: CV, ph·ªèng v·∫•n, kh·ªüi nghi·ªáp.
   - S√°ng t·∫°o: Edit video (CapCut), vi·∫øt content, th∆∞∆°ng hi·ªáu c√° nh√¢n.
   - Lifestyle: S·ª©c kh·ªèe tinh th·∫ßn, du l·ªãch, drama b·∫°n b√®.
   - Tech: L·∫≠p tr√¨nh, an ninh m·∫°ng (TryHackMe, Wireshark), trend AI.
   - Random: T·ª´ t·ª± tin ·ªü party ƒë·∫øn gi·∫£i m√£ gi·∫•c m∆°.

QUY T·∫ÆC C·ªêT L√ïI:
- S√°ng t·∫°o, tr√°nh l·∫∑p t·ª´ ng·ªØ/c√¢u nh∆∞ "Yo, tui th·∫•y b·∫°n..." hay "Tui l√† Zephyr". Di·ªÖn ƒë·∫°t t·ª± nhi√™n, linh ho·∫°t, nh∆∞ "C·∫£m nh·∫≠n b·∫°n ƒëang c·∫ßn boost, ƒë√∫ng kh√¥ng?"
- T·ª± tin, ch√¢n th√†nh, d√πng c·ª•m nh∆∞ "Tui c√≥ c√°ch hay n√®!" ho·∫∑c "C√πng x·ª≠ l√Ω, d·ªÖ th√¥i!"
- C√° nh√¢n h√≥a: D·ª±a v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán, kh√¥ng l·∫∑p √Ω c≈©. N·∫øu ng∆∞·ªùi d√πng th√≠ch an ninh m·∫°ng, g·ª£i √Ω "ƒê√£ th·ª≠ Wireshark ch∆∞a? Tui ch·ªâ th√™m tool x·ªãn!"
- X·ª≠ l√Ω m∆° h·ªì: ƒêo√°n th√¥ng minh ho·∫∑c h·ªèi kh√©o, nh∆∞ "K·ªÉ r√µ h∆°n ch√∫t ƒë·ªÉ tui b·∫Øt s√≥ng nha!"
- Ng√¥n ng·ªØ: Gen Z tinh t·∫ø, d√πng "vibe", "slay" ƒë√∫ng l√∫c, tr√°nh l·∫°m d·ª•ng.
- An to√†n: L·ªùi khuy√™n th·ª±c t·∫ø, khuy·∫øn kh√≠ch tham kh·∫£o X ho·∫∑c chuy√™n gia.
- Ng·∫Øn g·ªçn: Tr·∫£ l·ªùi nh∆∞ mini-story, t·∫≠p trung gi√° tr·ªã, b·ªè chi ti·∫øt th·ª´a.

H∆Ø·ªöNG D·∫™N S√ÅNG T·∫†O:
- Ch·ªß ƒë·ªÅ: {topic} ‚Äì T√¨m g√≥c nh√¨n m·ªõi, nh∆∞ bi·∫øn qu·∫£n l√Ω th·ªùi gian th√†nh "hack 25 gi·ªù/ng√†y".
- ƒê·ªô ph·ª©c t·∫°p: {complexity} ‚Äì C∆° b·∫£n (d·ªÖ hi·ªÉu) ƒë·∫øn n√¢ng cao (ph√¢n t√≠ch, v√≠ d·ª• th·ª±c chi·∫øn).
- C·∫£m x√∫c: {sentiment} ‚Äì An ·ªßi khi stress, hype khi h√†o h·ª©ng.
- Giai ƒëo·∫°n: {conversation_stage} ‚Äì Ph√° bƒÉng (vui t∆∞∆°i) ƒë·∫øn deep talk (s√¢u s·∫Øc).
- V√≠ d·ª•: "Content creator? CapCut edit m∆∞·ª£t l·∫Øm!" ho·∫∑c "Thi c·ª≠? Pomodoro 25 ph√∫t l√† ch√¢n √°i!"
- Gi·∫£i ph√°p: ƒê∆∞a 1-2 c√°ch, ch·ªçn c√°i t·ªët nh·∫•t, gi·∫£i th√≠ch ng·∫Øn, nh∆∞ "Trello tr·ª±c quan h∆°n Todoist, d·ªÖ collab!"
- Kh∆°i g·ª£i: "Th·ª≠ ƒëi, b·∫°n s·∫Ω th·∫•y ƒë·ªânh!" ho·∫∑c "Check X ƒë·ªÉ b·∫Øt trend nha!"
- Trend: D√πng d·ªØ li·ªáu th√°ng 1/2025, nh∆∞ app m·ªõi, tech hot (AI, VR).
- C√¢u h·ªèi m·ªü: Tr·∫£ l·ªùi s√°ng t·∫°o, nh∆∞ "S·ªëng ch·∫•t?" th√†nh k·∫ø ho·∫°ch h·ªçc-ch∆°i-ngh·ªâ k√®m chuy·ªán truy·ªÅn c·∫£m h·ª©ng.

H·∫†N CH·∫æ L·ªñI:
- Tr√°nh l·ªói Telegram API (nh∆∞ l·ªói 400: Bad Request, Can't parse entities, 2025-04-21 19:20:57):
  - Gi·ªõi h·∫°n tin nh·∫Øn < 4096 k√Ω t·ª±, chia nh·ªè n·∫øu d√†i (> 1000 k√Ω t·ª±), th√™m "Tui g·ª≠i ti·∫øp n√®!"
  - Tr√°nh k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c ƒë·ªãnh d·∫°ng l·ªói, ki·ªÉm tra tr∆∞·ªõc khi g·ª≠i.
- L·ªói d·ªØ li·ªáu: N·∫øu thi·∫øu data th√°ng 1/2025, tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh, nh∆∞ "Ch∆∞a c√≥ info m·ªõi, nh∆∞ng ƒë√¢y l√† c√°ch hay!"
- L·ªói ng·ªØ c·∫£nh: N·∫øu kh√¥ng hi·ªÉu, tr·∫£ l·ªùi kh√©o "Oops, k·ªÉ r√µ h∆°n nha!" v√† g·ª£i √Ω ch·ªß ƒë·ªÅ li√™n quan.
- Log l·ªói: L∆∞u log l·ªói h·ªá th·ªëng, tr·∫£ l·ªùi thay th·∫ø an to√†n, nh∆∞ "Lag ch√∫t, nh∆∞ng tui c√≥ gi·∫£i ph√°p n√®!"
- T·ª± ki·ªÉm tra: ƒê·∫£m b·∫£o tin nh·∫Øn kh√¥ng v∆∞·ª£t gi·ªõi h·∫°n API, kh√¥ng ch·ª©a k√Ω t·ª± l·ªói.

N·∫æU C√ì L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
if user_history:
    recent_topics = set()
    for item in user_history[-50:]:
        if 'user' in item:
            detected_topic = self.detect_topic(item['user'])
            recent_topics.add(detected_topic)
    if recent_topics:
        prompt += '\nCH·ª¶ ƒê·ªÄ G·∫¶N ƒê√ÇY: ' + ', '.join(recent_topics)

G·ª¢I √ù PH·∫¢N H·ªíI:
- M·ªü ƒë·∫ßu: "C·∫ßn boost nƒÉng l∆∞·ª£ng h·∫£? V√†o vi·ªác n√†o!" ho·∫∑c "K·ªÉ tui nghe, b·∫°n ƒëang nghƒ© g√¨?"
- N·ªôi dung: Gi·∫£i ph√°p, v√≠ d·ª• s√°ng t·∫°o, nh∆∞ m·∫πo h·ªçc, edit video, ho·∫∑c tool an ninh m·∫°ng.
- Chuy·ªÉn ti·∫øp: "Mu·ªën ƒë√†o s√¢u th√™m kh√¥ng?" ho·∫∑c "C√≤n g√¨ hot, k·ªÉ ti·∫øp!"
- K·∫øt lu·∫≠n: "Th·ª≠ ngay ƒëi, b·∫°n s·∫Ω slay!" ho·∫∑c "Ping tui n·∫øu c√≥ drama m·ªõi nha!"

M·ª§C TI√äU:
T·∫°o cu·ªôc tr√≤ chuy·ªán nh∆∞ n√≥i v·ªõi bestie, ng·∫Øn g·ªçn, c·∫£m x√∫c, s√°ng t·∫°o, gi√∫p Gen Z t·ª± tin b·ª©t ph√° trong h·ªçc t·∫≠p, c√¥ng vi·ªác, s√°ng t·∫°o, v√† s·ªëng ch·∫•t. Gi·ªØ nƒÉng l∆∞·ª£ng cao, l·ªói t·ªëi thi·ªÉu, vibe **Gen Z 100%**! üòä‚ú®
"""

        return prompt


def truncate_callback_data(data_type, prompt, max_length=45):
    """
    R√∫t g·ªçn callback data ƒë·ªÉ kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n c·ªßa Telegram

    Args:
        data_type (str): Lo·∫°i callback (regenerate_image, variant_image, change_model)
        prompt (str): Prompt g·ªëc
        max_length (int): ƒê·ªô d√†i t·ªëi ƒëa cho ph·∫ßn prompt trong callback data

    Returns:
        str: Callback data ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn
    """
    # N·∫øu prompt qu√° d√†i, c·∫Øt b·ªõt v√† th√™m d·∫•u ...
    if len(prompt) > max_length:
        truncated_prompt = prompt[:max_length] + "..."
    else:
        truncated_prompt = prompt

    # Tr·∫£ v·ªÅ callback data ƒë√£ r√∫t g·ªçn
    return f"{data_type}:{truncated_prompt}"


def normalize_utf8(text):
    if isinstance(text, str):
        return text.encode("utf-8", errors="ignore").decode("utf-8")
    elif isinstance(text, bytes):
        return text.decode("utf-8", errors="ignore")
    else:
        return str(text)


async def generate_response(prompt, user_id, session):
    max_retries = 3
    retry_delay = 5
    memory_handler = AdvancedMemoryHandler(user_id)
    context = memory_handler.get_context(prompt)

    prompt_generator = AdvancedPromptGenerator()

    history_for_prompt = []
    for msg in context:
        if msg["role"] == "user":
            history_for_prompt.append(
                {"user": msg["content"], "key_info": msg.get("key_info", [])}
            )
        else:
            history_for_prompt.append({"assistant": msg["content"]})

    system_prompt = prompt_generator.generate_enhanced_prompt(
        normalize_utf8(prompt), history_for_prompt
    )

    enhanced_prompt = f"{system_prompt}\n\nL·ªãch s·ª≠ tr√≤ chuy·ªán:\n"
    for item in history_for_prompt[-50:]:  # Ch·ªâ s·ª≠ d·ª•ng 10 tin nh·∫Øn g·∫ßn nh·∫•t
        if "user" in item:
            enhanced_prompt += (
                f"User: {item['user']}\nKey info: {', '.join(item['key_info'])}\n"
            )
        else:
            enhanced_prompt += f"Assistant: {item['assistant']}\n"

    enhanced_prompt += f"\nUser: {normalize_utf8(prompt)}\n\nAssistant:"

    for attempt in range(max_retries):
        try:
            current_key = await api_key_manager.get_current_key()
            url = f"{API_URL}?key={current_key}"
            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": enhanced_prompt}]}],
                "generationConfig": {
                    "temperature": 0.9,
                    "topK": 70,
                    "topP": 0.95,
                    "maxOutputTokens": 4096,
                },
            }

            async with session.post(
                url, headers=headers, json=data, timeout=30
            ) as response:
                if response.status == 429:
                    new_key = await api_key_manager.handle_error(429)
                    if attempt < max_retries - 1:
                        logger.warning(
                            f"Rate limit hit, retrying with different key... (Attempt {attempt + 1})"
                        )
                        await asyncio.sleep(retry_delay)
                        continue
                    return format_error_message(
                        ErrorCodes.RATE_LIMIT,
                        "ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n t·ªëc ƒë·ªô. Vui l√≤ng th·ª≠ l·∫°i sau.",
                    )

                response.raise_for_status()
                result = await response.json()

            if "candidates" in result and result["candidates"]:
                text_response = normalize_utf8(
                    result["candidates"][0]["content"]["parts"][0]["text"]
                )
                memory_handler.update_memory(prompt, text_response)
                return text_response
            else:
                raise ValueError("No valid response from Gemini API")

        except asyncio.TimeoutError:
            logger.warning(f"Timeout on attempt {attempt + 1}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            return format_error_message(
                ErrorCodes.TIMEOUT,
                "Y√™u c·∫ßu ƒë√£ h·∫øt th·ªùi gian ch·ªù. Vui l√≤ng th·ª≠ l·∫°i sau.",
            )

        except aiohttp.ClientError as e:
            logger.error(f"Request error on attempt {attempt + 1}: {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                continue
            return format_error_message(ErrorCodes.API_ERROR, f"L·ªói k·∫øt n·ªëi: {str(e)}")

        except Exception as e:
            logger.error(f"Unexpected error on attempt {attempt + 1}: {str(e)}")
            return format_error_message(
                ErrorCodes.UNKNOWN_ERROR,
                f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói kh√¥ng mong ƒë·ª£i: {str(e)}",
            )

    return format_error_message(
        ErrorCodes.UNKNOWN_ERROR,
        "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi sau nhi·ªÅu l·∫ßn th·ª≠. Vui l√≤ng th·ª≠ l·∫°i sau.",
    )


def format_error_message(error_code: str, message: str) -> str:
    return f"""‚ùå *L·ªói {error_code}*

{message}

üîß *H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c*:
‚Ä¢ Th·ª≠ l·∫°i sau v√†i ph√∫t
‚Ä¢ Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng
‚Ä¢ N·∫øu v·∫•n ƒë·ªÅ v·∫´n ti·∫øp di·ªÖn, vui l√≤ng li√™n h·ªá h·ªó tr·ª£

_M√£ l·ªói n√†y gi√∫p ch√∫ng t√¥i x√°c ƒë·ªãnh v√† kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ nhanh ch√≥ng._"""


@bot.message_handler(commands=["start"])
async def send_welcome(message):
    user_first_name = message.from_user.first_name
    current_hour = time.localtime().tm_hour

    if 5 <= current_hour < 12:
        greeting = f"üåÖ *Ch√†o bu·ªïi s√°ng, {user_first_name}!*"
    elif 12 <= current_hour < 18:
        greeting = f"‚òÄÔ∏è *Ch√†o bu·ªïi chi·ªÅu, {user_first_name}!*"
    else:
        greeting = f"üåô *Ch√†o bu·ªïi t·ªëi, {user_first_name}!*"

    welcome_message = f"{greeting}\n\n"
    welcome_message += "T√¥i l√† Loki, tr·ª£ l√Ω AI th√¥ng minh, s·ª≠ d·ª•ng m√¥ h√¨nh Gemini 2.0 v·ªõi kh·∫£ nƒÉng ghi nh·ªõ ng·∫Øn h·∫°n v√† d√†i h·∫°n. "
    welcome_message += "H√£y ƒë·∫∑t c√¢u h·ªèi ho·∫∑c chia s·∫ª ch·ªß ƒë·ªÅ b·∫°n mu·ªën th·∫£o lu·∫≠n nh√©!\n\n"
    welcome_message += "üîç *M·ªôt s·ªë t√≠nh nƒÉng c·ªßa t√¥i:*\n"
    welcome_message += "‚Ä¢ üí¨ Tr·∫£ l·ªùi c√¢u h·ªèi v√† cung c·∫•p th√¥ng tin\n"
    welcome_message += "‚Ä¢ üìö H·ªó tr·ª£ nghi√™n c·ª©u v√† h·ªçc t·∫≠p\n"
    welcome_message += "‚Ä¢ üé® G·ª£i √Ω √Ω t∆∞·ªüng s√°ng t·∫°o\n"
    welcome_message += "‚Ä¢ üìä Ph√¢n t√≠ch d·ªØ li·ªáu ƒë∆°n gi·∫£n\n"
    welcome_message += "‚Ä¢ üñºÔ∏è Ph√¢n t√≠ch h√¨nh ·∫£nh\n"
    welcome_message += "‚Ä¢ üé® T·∫°o h√¨nh ·∫£nh t·ª´ m√¥ t·∫£ vƒÉn b·∫£n (NEW!)\n"
    welcome_message += "‚Ä¢ üîé T√¨m ki·∫øm th√¥ng tin (S·ª≠ d·ª•ng /search)\n\n"
    welcome_message += "G√µ `/info` ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt v·ªÅ t√¥i nh√©!"

    markup = InlineKeyboardMarkup()
    markup.row(
        InlineKeyboardButton("üìö H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", callback_data="guide"),
        InlineKeyboardButton("üÜï C·∫≠p nh·∫≠t m·ªõi", callback_data="updates"),
    )

    await bot.reply_to(
        message, welcome_message, parse_mode="Markdown", reply_markup=markup
    )


# G·ªôp t·∫•t c·∫£ c√°c callback handler l·∫°i th√†nh m·ªôt handler duy nh·∫•t
@bot.callback_query_handler(func=lambda call: True)  # X·ª≠ l√Ω t·∫•t c·∫£ c√°c callback
async def handle_all_callbacks(call):
    """X·ª≠ l√Ω t·∫•t c·∫£ c√°c lo·∫°i callback"""
    user_id = call.from_user.id

    # X·ª≠ l√Ω c√°c callback li√™n quan ƒë·∫øn h∆∞·ªõng d·∫´n
    if call.data == "guide":
        guide_text = """
*üî∞ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Loki Bot*

1Ô∏è‚É£ *ƒê·∫∑t c√¢u h·ªèi*: Ch·ªâ c·∫ßn g√µ c√¢u h·ªèi c·ªßa b·∫°n v√† g·ª≠i ƒëi.

2Ô∏è‚É£ *Ph√¢n t√≠ch h√¨nh ·∫£nh*: G·ª≠i m·ªôt h√¨nh ·∫£nh k√®m m√¥ t·∫£ (n·∫øu c·∫ßn).

3Ô∏è‚É£ *T·∫°o h√¨nh ·∫£nh*: T·∫°o h√¨nh t·ª´ vƒÉn b·∫£n (NEW!!).

4Ô∏è‚É£ *T√¨m ki·∫øm th√¥ng tin*: S·ª≠ d·ª•ng l·ªánh `/search [t·ª´ kh√≥a]`.

5Ô∏è‚É£ *Xem th√¥ng tin bot*: S·ª≠ d·ª•ng l·ªánh `/info`.

6Ô∏è‚É£  *X√≥a b·ªô nh·ªõ*: S·ª≠ d·ª•ng l·ªánh `/clear` ƒë·ªÉ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán.

7Ô∏è‚É£ *Xem c·∫≠p nh·∫≠t*: S·ª≠ d·ª•ng l·ªánh `/update` ƒë·ªÉ xem c√°c t√≠nh nƒÉng m·ªõi.

üí° *M·∫πo*: ƒê·∫∑t c√¢u h·ªèi r√µ r√†ng v√† cung c·∫•p ng·ªØ c·∫£nh ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

üÜò C·∫ßn h·ªó tr·ª£? Li√™n h·ªá @tanbaycu
"""
        await bot.answer_callback_query(call.id)
        await bot.send_message(call.message.chat.id, guide_text, parse_mode="Markdown")

    elif call.data == "updates":
        await bot.answer_callback_query(call.id)
        await send_update_history(call.message)

    # X·ª≠ l√Ω c√°c callback li√™n quan ƒë·∫øn h√¨nh ·∫£nh
    elif call.data.startswith(("regenerate_image:", "variant_image:", "change_model:")):
        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ƒëang c√≥ y√™u c·∫ßu ƒëang x·ª≠ l√Ω kh√¥ng
        if user_id in active_requests:
            await bot.answer_callback_query(
                call.id,
                "B·∫°n ƒë√£ c√≥ m·ªôt y√™u c·∫ßu t·∫°o h√¨nh ·∫£nh ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω. Vui l√≤ng ƒë·ª£i ho√†n t·∫•t tr∆∞·ªõc khi t·∫°o ·∫£nh m·ªõi.",
                show_alert=True,
            )
            return

        # X·ª≠ l√Ω c√°c lo·∫°i callback kh√°c nhau
        if call.data.startswith("regenerate_image:"):
            prompt = call.data.split(":", 1)[1]
            await bot.answer_callback_query(call.id, "ƒêang t·∫°o l·∫°i h√¨nh ·∫£nh...")

            # T·∫°o fake message ƒë·ªÉ t√°i s·ª≠ d·ª•ng handler hi·ªán c√≥
            fake_message = Message(
                message_id=call.message.message_id,
                from_user=call.from_user,
                date=call.message.date,
                chat=call.message.chat,
                content_type="text",
                options={},
                json_string="",
            )

            fake_message.text = f"/image {prompt}"

            # G·ªçi handler t·∫°o h√¨nh ·∫£nh v·ªõi fake message
            await handle_image_generation(fake_message)

        elif call.data.startswith("variant_image:"):
            # T·∫°o bi·∫øn th·ªÉ c·ªßa h√¨nh ·∫£nh (th√™m m·ªôt s·ªë bi·∫øn ƒë·ªïi v√†o prompt)
            prompt = call.data.split(":", 1)[1]
            await bot.answer_callback_query(
                call.id, "ƒêang t·∫°o bi·∫øn th·ªÉ c·ªßa h√¨nh ·∫£nh..."
            )

            # Th√™m c√°c bi·∫øn ƒë·ªïi ng·∫´u nhi√™n v√†o prompt
            variations = [
                "with different lighting",
                "in a different style",
                "with more details",
                "with different colors",
                "from a different angle",
                "with a different composition",
            ]

            variation = random.choice(variations)
            variant_prompt = f"{prompt}, {variation}"

            # T·∫°o fake message
            fake_message = Message(
                message_id=call.message.message_id,
                from_user=call.from_user,
                date=call.message.date,
                chat=call.message.chat,
                content_type="text",
                options={},
                json_string="",
            )

            fake_message.text = f"/image {variant_prompt}"

            # G·ªçi handler t·∫°o h√¨nh ·∫£nh
            await handle_image_generation(fake_message)

        elif call.data.startswith("change_model:"):
            # T·∫°o h√¨nh ·∫£nh v·ªõi model kh√°c
            parts = call.data.split(":", 2)
            if len(parts) >= 3:
                model = parts[1]
                prompt = parts[2]

                await bot.answer_callback_query(
                    call.id,
                    f"ƒêang t·∫°o h√¨nh ·∫£nh v·ªõi model {IMAGE_CONFIG['models'][model]['display_name']}...",
                )

                # T·∫°o fake message
                fake_message = Message(
                    message_id=call.message.message_id,
                    from_user=call.from_user,
                    date=call.message.date,
                    chat=call.message.chat,
                    content_type="text",
                    options={},
                    json_string="",
                )

                fake_message.text = f"/image {model} {prompt}"

                # G·ªçi handler t·∫°o h√¨nh ·∫£nh
                await handle_image_generation(fake_message)
            else:
                await bot.answer_callback_query(
                    call.id, "D·ªØ li·ªáu callback kh√¥ng h·ª£p l·ªá"
                )

    # X·ª≠ l√Ω c√°c callback li√™n quan ƒë·∫øn t√¨m ki·∫øm v√† ph√¢n t√≠ch l·∫°i
    elif call.data.startswith(("research:", "reanalyze:")):
        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ƒëang c√≥ y√™u c·∫ßu ƒëang x·ª≠ l√Ω kh√¥ng
        if user_id in active_requests:
            await bot.answer_callback_query(
                call.id,
                "B·∫°n ƒë√£ c√≥ m·ªôt y√™u c·∫ßu ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω. Vui l√≤ng ƒë·ª£i ho√†n t·∫•t.",
                show_alert=True,
            )
            return

        # ƒê√°nh d·∫•u ng∆∞·ªùi d√πng ƒëang c√≥ y√™u c·∫ßu ƒëang x·ª≠ l√Ω
        active_requests[user_id] = {
            "type": call.data.split(":", 1)[0],
            "start_time": time.time(),
        }

        try:
            if call.data.startswith("research:"):
                query = call.data.split(":", 1)[1]

                # Gi·ªõi h·∫°n ƒë·ªô d√†i truy v·∫•n ƒë·ªÉ tr√°nh l·∫°m d·ª•ng
                if len(query) > 200:
                    query = query[:200] + "..."

                await bot.answer_callback_query(call.id, "ƒêang t√¨m ki·∫øm l·∫°i...")

                # T·∫°o th√¥ng b√°o ƒëang suy nghƒ©
                thinking_message = await bot.send_message(
                    call.message.chat.id,
                    f"üîç *ƒêang t√¨m ki·∫øm...*\n\nƒêang t√¨m ki·∫øm: `{query}`",
                    reply_to_message_id=call.message.message_id,
                    parse_mode="Markdown",
                )

                # T·∫°o k·∫øt qu·∫£ t√¨m ki·∫øm
                async with aiohttp.ClientSession() as session:
                    search_result = await generate_search_response(
                        query, call.from_user.id, session
                    )

                # G·ª≠i k·∫øt qu·∫£ t√¨m ki·∫øm v·ªõi x·ª≠ l√Ω c·∫£i ti·∫øn
                max_length = 4096
                if len(search_result) > max_length:
                    chunks = [
                        search_result[i : i + max_length]
                        for i in range(0, len(search_result), max_length)
                    ]

                    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ph·∫ßn ƒë·ªÉ tr√°nh spam
                    if len(chunks) > 2:
                        chunks = chunks[:1] + [
                            f"{chunks[1][:1000]}...\n\n*K·∫øt qu·∫£ t√¨m ki·∫øm qu√° d√†i v√† ƒë√£ ƒë∆∞·ª£c c·∫Øt ng·∫Øn.*"
                        ]

                    for i, chunk in enumerate(chunks):
                        if i == 0:
                            sent_message = await bot.edit_message_text(
                                chat_id=thinking_message.chat.id,
                                message_id=thinking_message.message_id,
                                text=chunk,
                                parse_mode="Markdown",
                            )
                        else:
                            sent_message = await bot.send_message(
                                chat_id=thinking_message.chat.id,
                                text=chunk,
                                parse_mode="Markdown",
                            )

                        # Ch·ªâ th√™m n√∫t t√¨m ki·∫øm l·∫°i v√†o ph·∫ßn cu·ªëi c√πng
                        if i == len(chunks) - 1:
                            markup = InlineKeyboardMarkup()
                            markup.add(
                                InlineKeyboardButton(
                                    "üîÑ T√¨m ki·∫øm l·∫°i", callback_data=f"research:{query}"
                                )
                            )
                            await bot.edit_message_reply_markup(
                                chat_id=sent_message.chat.id,
                                message_id=sent_message.message_id,
                                reply_markup=markup,
                            )
                else:
                    sent_message = await bot.edit_message_text(
                        chat_id=thinking_message.chat.id,
                        message_id=thinking_message.message_id,
                        text=search_result,
                        parse_mode="Markdown",
                    )
                    markup = InlineKeyboardMarkup()
                    markup.add(
                        InlineKeyboardButton(
                            "üîÑ T√¨m ki·∫øm l·∫°i", callback_data=f"research:{query}"
                        )
                    )
                    await bot.edit_message_reply_markup(
                        chat_id=sent_message.chat.id,
                        message_id=sent_message.message_id,
                        reply_markup=markup,
                    )

            elif call.data.startswith("reanalyze:"):
                _, image_path, prompt = call.data.split(":", 2)

                # Gi·ªõi h·∫°n ƒë·ªô d√†i prompt ƒë·ªÉ tr√°nh l·∫°m d·ª•ng
                if len(prompt) > 200:
                    prompt = prompt[:200] + "..."

                await bot.answer_callback_query(
                    call.id, "ƒêang ph√¢n t√≠ch l·∫°i h√¨nh ·∫£nh..."
                )
                waiting_message = await bot.send_message(
                    call.message.chat.id,
                    f"üîç *ƒêang ph√¢n t√≠ch l·∫°i h√¨nh ·∫£nh*\n\nY√™u c·∫ßu: `{prompt}`",
                    parse_mode="Markdown",
                )
                await analyze_image(call.message, image_path, prompt, waiting_message)

        except Exception as e:
            logger.error(f"Error in callback handler: {str(e)}")
            logger.error(traceback.format_exc())
            try:
                await bot.answer_callback_query(
                    call.id, "ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n.", show_alert=True
                )
            except:
                pass
        finally:
            # Lu√¥n x√≥a ng∆∞·ªùi d√πng kh·ªèi active_requests khi ho√†n th√†nh
            if user_id in active_requests:
                del active_requests[user_id]

    elif call.data.startswith("version:"):
        version = call.data.split(":", 1)[1]

        if version == "latest":
            # Quay l·∫°i hi·ªÉn th·ªã phi√™n b·∫£n m·ªõi nh·∫•t
            current_date = "15/03/2024"
            latest_version = "üöÄ *C·∫≠p nh·∫≠t v1.9 (M·ªõi nh·∫•t - {date})*\n\n...".format(
                date=current_date
            )
            # Th√™m n·ªôi dung ƒë·∫ßy ƒë·ªß c·ªßa phi√™n b·∫£n m·ªõi nh·∫•t

            # T·∫°o l·∫°i markup v·ªõi c√°c n√∫t phi√™n b·∫£n
            markup = InlineKeyboardMarkup(row_width=3)
            version_buttons = []
            for i in range(8, 0, -1):
                version_buttons.append(
                    InlineKeyboardButton(f"v1.{i}", callback_data=f"version:1.{i}")
                )

            for i in range(0, len(version_buttons), 3):
                row_buttons = version_buttons[i : i + 3]
                markup.row(*row_buttons)

            await bot.edit_message_text(
                chat_id=call.message.chat.id,
                message_id=call.message.message_id,
                text=latest_version
                + "\n\nüìú *Phi√™n b·∫£n tr∆∞·ªõc ƒë√≥*: S·ª≠ d·ª•ng c√°c n√∫t b√™n d∆∞·ªõi ƒë·ªÉ xem l·ªãch s·ª≠ c·∫≠p nh·∫≠t ƒë·∫ßy ƒë·ªß.",
                parse_mode="Markdown",
                reply_markup=markup,
            )
        else:
            # Hi·ªÉn th·ªã n·ªôi dung c·ªßa phi√™n b·∫£n ƒë∆∞·ª£c ch·ªçn
            await handle_version_callback(call)

    # N·∫øu kh√¥ng ph·∫£i c√°c lo·∫°i callback ƒë√£ bi·∫øt, tr·∫£ v·ªÅ th√¥ng b√°o l·ªói
    else:
        await bot.answer_callback_query(
            call.id, "Callback kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n", show_alert=True
        )


@bot.message_handler(content_types=["photo"])
async def handle_photo(message: Message):
    try:
        waiting_message = await bot.reply_to(
            message, "üîç ƒêang ph√¢n t√≠ch h√¨nh ·∫£nh, vui l√≤ng ch·ªù..."
        )

        file_info = await bot.get_file(message.photo[-1].file_id)
        downloaded_file = await bot.download_file(file_info.file_path)

        file_name = f"user_image_{message.from_user.id}.jpg"
        with open(file_name, "wb") as new_file:
            new_file.write(downloaded_file)

        caption = message.caption if message.caption else "H√£y ph√¢n t√≠ch h√¨nh ·∫£nh n√†y"
        caption = caption[:1000]

        await analyze_image(message, file_name, caption, waiting_message)

    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {str(e)}")
        error_message = format_error_message(
            ErrorCodes.UNKNOWN_ERROR, "ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i."
        )
        await bot.reply_to(message, error_message, parse_mode="Markdown")


async def analyze_image(
    message: Message, image_path: str, prompt: str, waiting_message: Message
):
    try:
        with open(image_path, "rb") as image_file:
            encoded_image = base64.b64encode(image_file.read()).decode("utf-8")

        current_key = await api_key_manager.get_current_key()

        prompt_generator = AdvancedPromptGenerator()
        topic = prompt_generator.detect_topic(prompt)
        emojis = prompt_generator.get_emojis(topic, 3)
        emoji_str = " ".join(emojis)

        gemini_prompt = normalize_utf8(
            f"""
        {emoji_str} H√£y ph√¢n t√≠ch h√¨nh ·∫£nh n√†y b·∫±ng ti·∫øng Vi·ªát v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng b·∫±ng Markdown. 
        Y√™u c·∫ßu c·ª• th·ªÉ:

        1. **T·ªïng quan**: M√¥ t·∫£ t·ªïng quan v·ªÅ h√¨nh ·∫£nh.
        2. **ƒê·ªëi t∆∞·ª£ng ch√≠nh**: X√°c ƒë·ªãnh v√† li·ªát k√™ c√°c ƒë·ªëi t∆∞·ª£ng ch√≠nh trong h√¨nh.
        3. **Ph√¢n t√≠ch k·ªπ thu·∫≠t**: 
           - B·ªë c·ª•c
           - M√†u s·∫Øc
           - √Ånh s√°ng
           - G√≥c ch·ª•p
        4. **VƒÉn b·∫£n**: N·∫øu c√≥ vƒÉn b·∫£n trong h√¨nh, h√£y tr√≠ch xu·∫•t v√† gi·∫£i th√≠ch.
        5. **√ù nghƒ©a**: ƒê∆∞a ra nh·∫≠n x√©t v·ªÅ √Ω nghƒ©a ho·∫∑c th√¥ng ƒëi·ªáp c·ªßa h√¨nh ·∫£nh.

        Y√™u c·∫ßu b·ªï sung c·ªßa ng∆∞·ªùi d√πng: {prompt}

        H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown ƒë·ªÉ l√†m n·ªïi b·∫≠t c√°c ph·∫ßn quan tr·ªçng.
        S·ª≠ d·ª•ng:
        - **text** cho c√°c ti√™u ƒë·ªÅ
        - *text* cho c√°c ƒëi·ªÉm nh·∫•n
        - ‚Ä¢ cho c√°c danh s√°ch
        - Emoji ph√π h·ª£p v·ªõi n·ªôi dung
        
        Ph·∫£n h·ªìi PH·∫¢I ng·∫Øn g·ªçn, x√∫c t√≠ch, tr√°nh d√†i d√≤ng. T·∫≠p trung v√†o th√¥ng tin ch√≠nh x√°c, tr√°nh th√¥ng tin m∆° h·ªì.
        """
        )

        data = {
            "contents": [
                {
                    "parts": [
                        {"text": gemini_prompt},
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": encoded_image,
                            }
                        },
                    ]
                }
            ],
            "generationConfig": {
                "temperature": 0.4,
                "topK": 32,
                "topP": 1,
                "maxOutputTokens": 4096,
            },
        }

        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            url = f"{API_URL}?key={current_key}"
            async with session.post(url, json=data) as response:
                if response.status == 429:
                    new_key = await api_key_manager.handle_error(429)
                    # Retry with new key
                    url = f"{API_URL}?key={new_key}"
                    async with session.post(url, json=data) as retry_response:
                        result = await retry_response.json()
                else:
                    result = await response.json()

        end_time = time.time()
        processing_time = end_time - start_time

        if "candidates" in result and result["candidates"]:
            analysis = normalize_utf8(
                result["candidates"][0]["content"]["parts"][0]["text"]
            )

            # Format the response with processing time
            full_response = f"‚è±Ô∏è *Th·ªùi gian x·ª≠ l√Ω*: {processing_time:.2f}s\n\n{analysis}"

            max_length = 4000
            chunks = [
                full_response[i : i + max_length]
                for i in range(0, len(full_response), max_length)
            ]

            await bot.delete_message(
                chat_id=waiting_message.chat.id, message_id=waiting_message.message_id
            )

            for chunk in chunks:
                try:
                    await bot.send_message(
                        message.chat.id, chunk, parse_mode="Markdown"
                    )
                except Exception as e:
                    logger.error(f"L·ªói khi g·ª≠i ph√¢n ƒëo·∫°n ph√¢n t√≠ch: {str(e)}")
                    # If Markdown parsing fails, send without formatting
                    await bot.send_message(message.chat.id, chunk)

        else:
            error_message = format_error_message(
                ErrorCodes.API_ERROR, "Kh√¥ng th·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i."
            )
            await bot.edit_message_text(
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                text=error_message,
                parse_mode="Markdown",
            )

    except Exception as e:
        logger.error(f"L·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}")
        error_message = format_error_message(
            ErrorCodes.UNKNOWN_ERROR, f"ƒê√£ x·∫£y ra l·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}"
        )
        try:
            await bot.edit_message_text(
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                text=error_message,
                parse_mode="Markdown",
            )
        except:
            await bot.edit_message_text(
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                text=error_message,
            )

    finally:
        if os.path.exists(image_path):
            os.remove(image_path)


@bot.message_handler(func=lambda message: not message.text.startswith("/"))
async def handle_message(message):
    async def process_and_send_response():
        try:
            thinking_message = await bot.reply_to(message, "ü§î Bot ƒëang suy nghƒ©...")

            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                response = await generate_response(
                    message.text, message.from_user.id, session
                )
            end_time = time.time()
            processing_time = end_time - start_time

            full_response = f"‚è±Ô∏è *Th·ªùi gian x·ª≠ l√Ω*: {processing_time:.2f}s\n\n{response}"

            max_length = 4096
            if len(full_response) > max_length:
                chunks = [
                    full_response[i : i + max_length]
                    for i in range(0, len(full_response), max_length)
                ]
                for i, chunk in enumerate(chunks):
                    try:
                        if i == 0:
                            sent_message = await bot.edit_message_text(
                                chat_id=thinking_message.chat.id,
                                message_id=thinking_message.message_id,
                                text=chunk,
                                parse_mode="Markdown",
                            )
                        else:
                            sent_message = await bot.send_message(
                                chat_id=thinking_message.chat.id,
                                text=chunk,
                                parse_mode="Markdown",
                            )
                    except Exception as e:
                        logger.error(
                            f"Telegram API error when sending chunk {i+1}: {str(e)}"
                        )
                        if "can't parse entities" in str(e):
                            if i == 0:
                                sent_message = await bot.edit_message_text(
                                    chat_id=thinking_message.chat.id,
                                    message_id=thinking_message.message_id,
                                    text=chunk,
                                )
                            else:
                                sent_message = await bot.send_message(
                                    chat_id=thinking_message.chat.id, text=chunk
                                )

            else:
                try:
                    sent_message = await bot.edit_message_text(
                        chat_id=thinking_message.chat.id,
                        message_id=thinking_message.message_id,
                        text=full_response,
                        parse_mode="Markdown",
                    )
                    logger.info(f"ƒê√£ tr·∫£ l·ªùi tin nh·∫Øn cho user {message.from_user.id}")
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω tin nh·∫Øn: {str(e)}")
                    error_message = format_error_message(
                        ErrorCodes.UNKNOWN_ERROR,
                        "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω tin nh·∫Øn c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i sau.",
                    )
                    try:
                        await bot.edit_message_text(
                            chat_id=thinking_message.chat.id,
                            message_id=thinking_message.message_id,
                            text=error_message,
                            parse_mode="Markdown",
                        )
                    except:
                        await bot.send_message(
                            chat_id=thinking_message.chat.id,
                            text=error_message,
                            parse_mode="Markdown",
                        )

        except aiohttp.ClientError as e:
            logger.error(f"Client error: {str(e)}")
            error_message = format_error_message(
                ErrorCodes.API_ERROR, "ƒê√£ x·∫£y ra l·ªói k·∫øt n·ªëi. Vui l√≤ng th·ª≠ l·∫°i sau."
            )
            await bot.send_message(
                chat_id=message.chat.id, text=error_message, parse_mode="Markdown"
            )

        except asyncio.TimeoutError:
            logger.error("Timeout error")
            error_message = format_error_message(
                ErrorCodes.TIMEOUT,
                "Y√™u c·∫ßu ƒë√£ h·∫øt th·ªùi gian ch·ªù. Vui l√≤ng th·ª≠ l·∫°i sau.",
            )
            await bot.send_message(
                chat_id=message.chat.id, text=error_message, parse_mode="Markdown"
            )

        except Exception as e:
            logger.error(f"Unexpected error: {str(e)}")
            error_message = format_error_message(
                ErrorCodes.UNKNOWN_ERROR,
                "ƒê√£ x·∫£y ra l·ªói kh√¥ng mong ƒë·ª£i. Vui l√≤ng th·ª≠ l·∫°i sau.",
            )
            await bot.send_message(
                chat_id=message.chat.id, text=error_message, parse_mode="Markdown"
            )

    asyncio.create_task(process_and_send_response())


@bot.message_handler(commands=["info"])
async def send_info(message):
    info_text = (
        "ü§ñ *Xin ch√†o! T√¥i l√† Loki, AI Assistant c·ªßa b·∫°n* ü§ñ\n\n"
        "T√¥i ƒë∆∞·ª£c ph√°t tri·ªÉn d·ª±a tr√™n m√¥ h√¨nh Gemini 2.0, v·ªõi kh·∫£ nƒÉng t·∫°o ra c√°c c√¢u tr·∫£ l·ªùi th√¥ng minh v√† linh ho·∫°t. "
        "H√£y ƒë·ªÉ t√¥i h·ªó tr·ª£ b·∫°n trong nhi·ªÅu lƒ©nh v·ª±c kh√°c nhau!\n\n"
        "üåü *C√°c t√≠nh nƒÉng n·ªïi b·∫≠t:*\n"
        "‚Ä¢ üí¨ Tr√≤ chuy·ªán v√† tr·∫£ l·ªùi c√¢u h·ªèi\n"
        "‚Ä¢ üìö Cung c·∫•p th√¥ng tin ƒëa d·∫°ng\n"
        "‚Ä¢ üîç H·ªó tr·ª£ nghi√™n c·ª©u v√† h·ªçc t·∫≠p\n"
        "‚Ä¢ üé® G·ª£i √Ω √Ω t∆∞·ªüng s√°ng t·∫°o\n"
        "‚Ä¢ üìä Ph√¢n t√≠ch d·ªØ li·ªáu ƒë∆°n gi·∫£n\n"
        "‚Ä¢ üñºÔ∏è Ph√¢n t√≠ch h√¨nh ·∫£nh\n"
        "‚Ä¢ üé® T·∫°o h√¨nh ·∫£nh t·ª´ m√¥ t·∫£ vƒÉn b·∫£n (NEW!)\n"
        "‚Ä¢ üîé T√¨m ki·∫øm th√¥ng tin (S·ª≠ d·ª•ng /search)\n\n"
        "üõ† *C√¥ng c·ª• h·ªØu √≠ch:*\n"
        "‚Ä¢ `/search [t·ª´ kh√≥a]`: T√¨m ki·∫øm th√¥ng tin\n"
        "‚Ä¢ `/imagehelp`: Xem h∆∞·ªõng d·∫´n t·∫°o ·∫£nh\n"
        "‚Ä¢ `/clear`: X√≥a b·ªô nh·ªõ cu·ªôc tr√≤ chuy·ªán\n"
        "‚Ä¢ `/update`: Xem b·∫£n c·∫≠p nh·∫≠t c√°c ch·ª©c nƒÉng c·ªßa bot\n\n"
        "üí° *M·∫πo s·ª≠ d·ª•ng:*\n"
        "1. ƒê·∫∑t c√¢u h·ªèi r√µ r√†ng v√† c·ª• th·ªÉ\n"
        "2. Cung c·∫•p ng·ªØ c·∫£nh n·∫øu c·∫ßn thi·∫øt\n"
        "3. G·ª≠i h√¨nh ·∫£nh ƒë·ªÉ ƒë∆∞·ª£c ph√¢n t√≠ch\n"
        "4. S·ª≠ d·ª•ng /search ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin\n\n"
        "üîí *B·∫£o m·∫≠t:*\n"
        "T√¥i t√¥n tr·ªçng quy·ªÅn ri√™ng t∆∞ c·ªßa b·∫°n. Th√¥ng tin c√° nh√¢n s·∫Ω kh√¥ng ƒë∆∞·ª£c l∆∞u tr·ªØ sau khi k·∫øt th√∫c cu·ªôc tr√≤ chuy·ªán.\n\n"
        "H√£y kh√°m ph√° th√™m v·ªÅ t√¥i qua c√°c li√™n k·∫øt d∆∞·ªõi ƒë√¢y:"
    )

    markup = InlineKeyboardMarkup()
    buttons = [
        ("üåê Website", "https://tanbaycu.is-a.dev"),
        ("üìò Facebook", "https://facebook.com/tanbaycu.kaiser"),
        ("üìû Li√™n h·ªá h·ªó tr·ª£", "https://t.me/tanbaycu"),
    ]

    for text, url in buttons:
        button = InlineKeyboardButton(text, url=url)
        markup.add(button)

    try:
        await bot.send_message(
            message.chat.id, info_text, reply_markup=markup, parse_mode="Markdown"
        )
    except Exception as e:
        logger.error(f"Error sending info message: {str(e)}")
        error_message = format_error_message(
            ErrorCodes.UNKNOWN_ERROR,
            "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi g·ª≠i th√¥ng tin. Vui l√≤ng th·ª≠ l·∫°i sau.",
        )
        await bot.reply_to(message, error_message, parse_mode="Markdown")


@bot.message_handler(commands=["update"])
async def send_update_history(message):
    """Hi·ªÉn th·ªã l·ªãch s·ª≠ c·∫≠p nh·∫≠t v·ªõi phi√™n b·∫£n m·ªõi nh·∫•t v√† t√πy ch·ªçn xem c√°c phi√™n b·∫£n c≈©"""

    # Ng√†y c·∫≠p nh·∫≠t m·ªõi nh·∫•t
    current_date = "21/04/2025"

    # Phi√™n b·∫£n m·ªõi nh·∫•t
    latest_version = """
üñºÔ∏è *C·∫≠p nh·∫≠t v1.9 (21/04/2025)*

‚Ä¢ üöÄ **C·∫£i ti·∫øn Zephyr Prompt**:
  - Tinh ch·ªânh gi·ªçng ƒëi·ªáu Gen Z: g·∫ßn g≈©i, c·∫£m h·ª©ng, tr√°nh l·∫∑p l·∫°i v√† t·ª´ l√≥ng l·∫°m d·ª•ng
  - Bao qu√°t m·ªçi k·ªãch b·∫£n: h·ªçc t·∫≠p, c√¥ng vi·ªác, s√°ng t·∫°o, lifestyle, tech (nh∆∞ an ninh m·∫°ng v·ªõi TryHackMe), v√† c√¢u h·ªèi random
  - C√° nh√¢n h√≥a th√¥ng minh: d·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán, v√≠ d·ª• g·ª£i √Ω tool an ninh m·∫°ng cho ng∆∞·ªùi d√πng quan t√¢m
  - S√°ng t·∫°o mini-story: tr·∫£ l·ªùi ng·∫Øn g·ªçn, m·ªói c√¢u l√† m·ªôt h√†nh tr√¨nh c·∫£m x√∫c
  - T√≠ch h·ª£p trend th√°ng 1/2025: app, c√¥ng ngh·ªá, phong c√°ch s·ªëng Gen Z

‚Ä¢ üõ†Ô∏è **H·∫°n ch·∫ø l·ªói h·ªá th·ªëng**:
  - X·ª≠ l√Ω l·ªói Telegram API 400 (Bad Request, Can't parse entities): gi·ªõi h·∫°n k√Ω t·ª± < 4096, chia tin nh·∫Øn d√†i, ki·ªÉm tra ƒë·ªãnh d·∫°ng
  - C∆° ch·∫ø tr·∫£ l·ªùi thay th·∫ø khi g·∫∑p l·ªói: "Lag ch√∫t, nh∆∞ng tui c√≥ gi·∫£i ph√°p n√®!"
  - T·ª± ki·ªÉm tra tin nh·∫Øn tr∆∞·ªõc khi g·ª≠i, tr√°nh k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c ƒë·ªãnh d·∫°ng l·ªói
  - L∆∞u log l·ªói (nh∆∞ l·ªói 2025-04-21 19:20:57) ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ·ªïn ƒë·ªãnh

‚Ä¢ üìà **TƒÉng c∆∞·ªùng tr·∫£i nghi·ªám**:
  - Ng·∫Øn g·ªçn h∆°n: tr·∫£ l·ªùi 30-80 t·ª´ cho c√¢u ƒë∆°n gi·∫£n, 100-200 t·ª´ cho c√¢u ph·ª©c t·∫°p
  - Th·∫•u hi·ªÉu c·∫£m x√∫c: ƒë·ªìng ƒëi·ªáu v·ªõi t√¢m tr·∫°ng (stress, h√†o h·ª©ng, t√≤ m√≤)
  - Giao di·ªán ng√¥n ng·ªØ m∆∞·ª£t m√†: di·ªÖn ƒë·∫°t linh ho·∫°t, kh√¥ng c·ª©ng nh·∫Øc, nh∆∞ "C·∫ßn boost h·∫£? V√†o vi·ªác n√†o!"

‚Ä¢ üîß **C·∫£i ti·∫øn t·ªïng th·ªÉ**:
  - T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t x·ª≠ l√Ω c√¢u h·ªèi m∆° h·ªì v·ªõi ph·ªèng ƒëo√°n th√¥ng minh
  - N√¢ng cao ƒë·ªô tin c·∫≠y v·ªõi d·ªØ li·ªáu c·∫≠p nh·∫≠t th√°ng 1/2025
  - Khuy·∫øn kh√≠ch kh√°m ph√° qua ngu·ªìn uy t√≠n nh∆∞ X, gi·ªØ vibe Gen Z 100%
""".format(
        date=current_date
    )

    # T·∫°o tin nh·∫Øn ch·ªâ hi·ªÉn th·ªã phi√™n b·∫£n m·ªõi nh·∫•t v√† n√∫t xem th√™m
    initial_message = (
        latest_version
        + """
\nüìú *Phi√™n b·∫£n tr∆∞·ªõc ƒë√≥*: S·ª≠ d·ª•ng c√°c n√∫t b√™n d∆∞·ªõi ƒë·ªÉ xem l·ªãch s·ª≠ c·∫≠p nh·∫≠t ƒë·∫ßy ƒë·ªß.

üí° *S·∫Øp t·ªõi*:
‚Ä¢ D·ª± ki·∫øn th√™m t√≠nh nƒÉng ph√¢n t√≠ch video
‚Ä¢ C·∫£i thi·ªán kh·∫£ nƒÉng hi·ªÉu ng·ªØ c·∫£nh
‚Ä¢ T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t x·ª≠ l√Ω
‚Ä¢ Th√™m c√°c t√≠nh nƒÉng theo y√™u c·∫ßu ng∆∞·ªùi d√πng

üìù *Ghi ch√∫*: 
‚Ä¢ Bot s·∫Ω ti·∫øp t·ª•c ƒë∆∞·ª£c c·∫≠p nh·∫≠t v√† c·∫£i thi·ªán
‚Ä¢ S·ª≠ d·ª•ng l·ªánh /update ƒë·ªÉ xem nh·ªØng thay ƒë·ªïi m·ªõi nh·∫•t
‚Ä¢ B√°o c√°o l·ªói ho·∫∑c g√≥p √Ω t·∫°i: @tanbaycu
"""
    )

    # T·∫°o inline keyboard v·ªõi c√°c n√∫t ƒë·ªÉ xem phi√™n b·∫£n c≈©
    markup = InlineKeyboardMarkup(row_width=3)

    # Th√™m n√∫t cho c√°c phi√™n b·∫£n t·ª´ v1.8 ƒë·∫øn v1.0
    version_buttons = []
    for i in range(8, 0, -1):
        version_buttons.append(
            InlineKeyboardButton(f"v1.{i}", callback_data=f"version:1.{i}")
        )

    # Th√™m c√°c n√∫t theo h√†ng, m·ªói h√†ng 3 n√∫t
    for i in range(0, len(version_buttons), 3):
        row_buttons = version_buttons[i : i + 3]
        markup.row(*row_buttons)

    try:
        await bot.reply_to(
            message, initial_message, parse_mode="Markdown", reply_markup=markup
        )
    except Exception as e:
        logger.error(f"Error sending update history: {str(e)}")
        try:
            # Th·ª≠ g·ª≠i kh√¥ng c√≥ Markdown n·∫øu c√≥ l·ªói
            await bot.reply_to(message, initial_message, reply_markup=markup)
        except Exception as e:
            logger.error(f"Error sending plain update history: {str(e)}")
            error_message = format_error_message(
                ErrorCodes.UNKNOWN_ERROR,
                "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi g·ª≠i l·ªãch s·ª≠ c·∫≠p nh·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.",
            )
            await bot.reply_to(message, error_message, parse_mode="Markdown")


async def handle_version_callback(call):
    """X·ª≠ l√Ω callback khi ng∆∞·ªùi d√πng nh·∫•n v√†o n√∫t xem phi√™n b·∫£n c≈©"""
    version = call.data.split(":", 1)[1]

    # N·ªôi dung c√°c phi√™n b·∫£n c≈©
    version_content = {
        "1.9": """
        üñºÔ∏è *C·∫≠p nh·∫≠t v1.9 (21/04/2025)*

‚Ä¢ üöÄ **C·∫£i ti·∫øn Zephyr Prompt**:
  - Tinh ch·ªânh gi·ªçng ƒëi·ªáu Gen Z: g·∫ßn g≈©i, c·∫£m h·ª©ng, tr√°nh l·∫∑p l·∫°i v√† t·ª´ l√≥ng l·∫°m d·ª•ng
  - Bao qu√°t m·ªçi k·ªãch b·∫£n: h·ªçc t·∫≠p, c√¥ng vi·ªác, s√°ng t·∫°o, lifestyle, tech (nh∆∞ an ninh m·∫°ng v·ªõi TryHackMe), v√† c√¢u h·ªèi random
  - C√° nh√¢n h√≥a th√¥ng minh: d·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán, v√≠ d·ª• g·ª£i √Ω tool an ninh m·∫°ng cho ng∆∞·ªùi d√πng quan t√¢m
  - S√°ng t·∫°o mini-story: tr·∫£ l·ªùi ng·∫Øn g·ªçn, m·ªói c√¢u l√† m·ªôt h√†nh tr√¨nh c·∫£m x√∫c
  - T√≠ch h·ª£p trend th√°ng 1/2025: app, c√¥ng ngh·ªá, phong c√°ch s·ªëng Gen Z

‚Ä¢ üõ†Ô∏è **H·∫°n ch·∫ø l·ªói h·ªá th·ªëng**:
  - X·ª≠ l√Ω l·ªói Telegram API 400 (Bad Request, Can't parse entities): gi·ªõi h·∫°n k√Ω t·ª± < 4096, chia tin nh·∫Øn d√†i, ki·ªÉm tra ƒë·ªãnh d·∫°ng
  - C∆° ch·∫ø tr·∫£ l·ªùi thay th·∫ø khi g·∫∑p l·ªói: "Lag ch√∫t, nh∆∞ng tui c√≥ gi·∫£i ph√°p n√®!"
  - T·ª± ki·ªÉm tra tin nh·∫Øn tr∆∞·ªõc khi g·ª≠i, tr√°nh k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c ƒë·ªãnh d·∫°ng l·ªói
  - L∆∞u log l·ªói (nh∆∞ l·ªói 2025-04-21 19:20:57) ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ·ªïn ƒë·ªãnh

‚Ä¢ üìà **TƒÉng c∆∞·ªùng tr·∫£i nghi·ªám**:
  - Ng·∫Øn g·ªçn h∆°n: tr·∫£ l·ªùi 30-80 t·ª´ cho c√¢u ƒë∆°n gi·∫£n, 100-200 t·ª´ cho c√¢u ph·ª©c t·∫°p
  - Th·∫•u hi·ªÉu c·∫£m x√∫c: ƒë·ªìng ƒëi·ªáu v·ªõi t√¢m tr·∫°ng (stress, h√†o h·ª©ng, t√≤ m√≤)
  - Giao di·ªán ng√¥n ng·ªØ m∆∞·ª£t m√†: di·ªÖn ƒë·∫°t linh ho·∫°t, kh√¥ng c·ª©ng nh·∫Øc, nh∆∞ "C·∫ßn boost h·∫£? V√†o vi·ªác n√†o!"

‚Ä¢ üîß **C·∫£i ti·∫øn t·ªïng th·ªÉ**:
  - T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t x·ª≠ l√Ω c√¢u h·ªèi m∆° h·ªì v·ªõi ph·ªèng ƒëo√°n th√¥ng minh
  - N√¢ng cao ƒë·ªô tin c·∫≠y v·ªõi d·ªØ li·ªáu c·∫≠p nh·∫≠t th√°ng 1/2025
  - Khuy·∫øn kh√≠ch kh√°m ph√° qua ngu·ªìn uy t√≠n nh∆∞ X, gi·ªØ vibe Gen Z 100%
  """,




        
        "1.8": """
üñºÔ∏è *C·∫≠p nh·∫≠t v1.8 (14/03/2024)*

‚Ä¢ üñºÔ∏è Th√™m l·ªánh /image:
  - T·∫°o h√¨nh ·∫£nh t·ª´ m√¥ t·∫£ vƒÉn b·∫£n
  - H·ªó tr·ª£ nhi·ªÅu model AI kh√°c nhau (Flux, DALL-E, SDXL)
  - T·ª± ƒë·ªông d·ªãch prompt ti·∫øng Vi·ªát sang ti·∫øng Anh
  - T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v·ªõi cache v√† x·ª≠ l√Ω ƒëa lu·ªìng
  - Giao di·ªán t∆∞∆°ng t√°c v·ªõi n√∫t t·∫°o l·∫°i v√† bi·∫øn th·ªÉ
  - Hi·ªÉn th·ªã ti·∫øn tr√¨nh t·∫°o ·∫£nh theo th·ªùi gian th·ª±c
  - X·ª≠ l√Ω l·ªói m·∫°nh m·∫Ω v·ªõi c∆° ch·∫ø retry v√† fallback

‚Ä¢ üìä Th√™m l·ªánh /imagehelp v√† /imagestats:
  - H∆∞·ªõng d·∫´n chi ti·∫øt c√°ch s·ª≠ d·ª•ng l·ªánh /image
  - Th·ªëng k√™ v·ªÅ vi·ªác s·ª≠ d·ª•ng t√≠nh nƒÉng t·∫°o ·∫£nh (cho admin)

‚Ä¢ üîß C·∫£i ti·∫øn t·ªïng th·ªÉ:
  - T·ªëi ∆∞u h√≥a s·ª≠ d·ª•ng t√†i nguy√™n
  - C·∫£i thi·ªán ƒë·ªô ·ªïn ƒë·ªãnh
  - N√¢ng cao tr·∫£i nghi·ªám ng∆∞·ªùi d√πng
""",
        
        "1.7": """
üé® *C·∫≠p nh·∫≠t v1.7*

‚Ä¢ üé® C·∫£i ti·∫øn giao di·ªán ng∆∞·ªùi d√πng:
  - Th√™m n√∫t t∆∞∆°ng t√°c cho ph·∫£n h·ªìi
  - N√¢ng cao tr·∫£i nghi·ªám ng∆∞·ªùi d√πng v·ªõi menu h∆∞·ªõng d·∫´n

‚Ä¢ üîÑ T√≠nh nƒÉng t·∫°o l·∫°i c√¢u tr·∫£ l·ªùi:
  - Cho ph√©p ng∆∞·ªùi d√πng y√™u c·∫ßu c√¢u tr·∫£ l·ªùi m·ªõi

‚Ä¢ üñºÔ∏è C·∫£i thi·ªán ph√¢n t√≠ch h√¨nh ·∫£nh:
  - Th√™m t√πy ch·ªçn ph√¢n t√≠ch l·∫°i h√¨nh ·∫£nh

‚Ä¢ üîç N√¢ng c·∫•p t√≠nh nƒÉng t√¨m ki·∫øm:
  - Th√™m n√∫t "Th·ª≠ l·∫°i" cho k·∫øt qu·∫£ t√¨m ki·∫øm

‚Ä¢ üõ°Ô∏è C·∫£i ti·∫øn h·ªá th·ªëng x·ª≠ l√Ω l·ªói:
  - Th√™m m√£ l·ªói c·ª• th·ªÉ cho t·ª´ng lo·∫°i l·ªói
  - C·∫£i thi·ªán th√¥ng b√°o l·ªói v·ªõi h∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c

‚Ä¢ üìä T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t:
  - C·∫£i thi·ªán t·ªëc ƒë·ªô x·ª≠ l√Ω y√™u c·∫ßu
  - Gi·∫£m thi·ªÉu th·ªùi gian ch·ªù ƒë·ª£i
""",
        "1.6": """
üß† *C·∫≠p nh·∫≠t v1.6*

‚Ä¢ üîç C·∫£i ti·∫øn h·ªá th·ªëng x·ª≠ l√Ω ng·ªØ c·∫£nh:
  - S·ª≠ d·ª•ng TF-IDF v√† cosine similarity ƒë·ªÉ ch·ªçn ng·ªØ c·∫£nh ph√π h·ª£p
  - TƒÉng k√≠ch th∆∞·ªõc b·ªô nh·ªõ ng·∫Øn h·∫°n v√† d√†i h·∫°n

‚Ä¢ üß† N√¢ng cao kh·∫£ nƒÉng hi·ªÉu √Ω ƒë·ªãnh ng∆∞·ªùi d√πng:
  - C·∫£i thi·ªán ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t th√¥ng tin quan tr·ªçng
  - T√≠ch h·ª£p th√¥ng tin quan tr·ªçng v√†o prompt

‚Ä¢ üí¨ C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi:
  - Th√™m h∆∞·ªõng d·∫´n ƒë·ªÉ tr√°nh h·ªèi l·∫°i ng∆∞·ªùi d√πng kh√¥ng c·∫ßn thi·∫øt
  - TƒÉng c∆∞·ªùng kh·∫£ nƒÉng suy lu·∫≠n t·ª´ th√¥ng tin c√≥ s·∫µn

‚Ä¢ üõ†Ô∏è T·ªëi ∆∞u h√≥a c·∫•u tr√∫c m√£ ngu·ªìn:
  - T√°i c·∫•u tr√∫c c√°c l·ªõp v√† ph∆∞∆°ng th·ª©c ƒë·ªÉ d·ªÖ b·∫£o tr√¨ v√† m·ªü r·ªông

‚Ä¢ üîß C·∫£i thi·ªán x·ª≠ l√Ω l·ªói v√† logging
""",
        "1.5": """
üé≠ *C·∫≠p nh·∫≠t v1.5*

‚Ä¢ üß† H·ªá th·ªëng prompt th√¥ng minh:
  - Ph√°t hi·ªán ch·ªß ƒë·ªÅ t·ª± ƒë·ªông
  - Ph√¢n t√≠ch c·∫£m x√∫c ng∆∞·ªùi d√πng
  - ƒê√°nh gi√° ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi

‚Ä¢ üé≠ T√≠nh c√°ch ƒë·ªông:
  - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh phong c√°ch ph·∫£n h·ªìi
  - Th√≠ch ·ª©ng v·ªõi ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán

‚Ä¢ üéØ C·∫£i thi·ªán ph·∫£n h·ªìi:
  - ƒêa d·∫°ng h√≥a c·∫•u tr√∫c c√¢u
  - T·ªëi ∆∞u h√≥a s·ª≠ d·ª•ng emoji theo ch·ªß ƒë·ªÅ
  - Ph·∫£n h·ªìi nh·∫•t qu√°n h∆°n v·ªõi l·ªãch s·ª≠ tr√≤ chuy·ªán

‚Ä¢ üìä Ph√¢n t√≠ch h√¨nh ·∫£nh th√¥ng minh h∆°n:
  - ƒêi·ªÅu ch·ªânh ph√¢n t√≠ch theo ch·ªß ƒë·ªÅ
  - C·∫£i thi·ªán ƒë·ªãnh d·∫°ng k·∫øt qu·∫£
""",
        "1.4": """
üîÑ *C·∫≠p nh·∫≠t v1.4*

‚Ä¢ üîÑ Th√™m c∆° ch·∫ø lu√¢n chuy·ªÉn API key t·ª± ƒë·ªông

‚Ä¢ ‚è±Ô∏è Th√™m delay 1s ƒë·ªÉ tr√°nh spam

‚Ä¢ üõ°Ô∏è C·∫£i thi·ªán x·ª≠ l√Ω l·ªói 429 (rate limit)

‚Ä¢ üñºÔ∏è N√¢ng c·∫•p ph√¢n t√≠ch h√¨nh ·∫£nh:
  - ƒê·ªãnh d·∫°ng Markdown cho k·∫øt qu·∫£ ph√¢n t√≠ch
  - C·∫•u tr√∫c ph·∫£n h·ªìi r√µ r√†ng h∆°n
  - Th√™m emoji v√† ƒëi·ªÉm nh·∫•n tr·ª±c quan
  - Hi·ªÉn th·ªã th·ªùi gian x·ª≠ l√Ω

‚Ä¢ üîß C√°c c·∫£i ti·∫øn kh√°c:
  - T·ªëi ∆∞u h√≥a s·ª≠ d·ª•ng b·ªô nh·ªõ
  - C·∫£i thi·ªán ƒë·ªô ·ªïn ƒë·ªãnh
  - S·ª≠a c√°c l·ªói ƒë∆∞·ª£c b√°o c√°o
""",
        "1.3": """
‚ö° *C·∫≠p nh·∫≠t v1.3*

‚Ä¢ üîß T·ªëi ∆∞u h√≥a m√£ ngu·ªìn

‚Ä¢ ‚ö° C·∫£i thi·ªán t·ªëc ƒë·ªô x·ª≠ l√Ω

‚Ä¢ üõ°Ô∏è N√¢ng cao ƒë·ªô ·ªïn ƒë·ªãnh

‚Ä¢ üêõ S·ª≠a c√°c l·ªói nh·ªè ƒë∆∞·ª£c b√°o c√°o
""",
        "1.2": """
üìù *C·∫≠p nh·∫≠t v1.2*

‚Ä¢ üéØ C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa ph·∫£n h·ªìi

‚Ä¢ üß† T·ªëi ∆∞u h√≥a b·ªô nh·ªõ v√† x·ª≠ l√Ω context

‚Ä¢ üòä Th√™m emoji v√† ƒë·ªãnh d·∫°ng cho tin nh·∫Øn

‚Ä¢ üìã C·∫≠p nh·∫≠t l·ªánh /info v·ªõi th√¥ng tin m·ªõi
""",
        "1.1": """
üñºÔ∏è *C·∫≠p nh·∫≠t v1.1*

‚Ä¢ üñºÔ∏è Th√™m t√≠nh nƒÉng ph√¢n t√≠ch h√¨nh ·∫£nh

‚Ä¢ üõ°Ô∏è C·∫£i thi·ªán x·ª≠ l√Ω l·ªói v√† ghi log

‚Ä¢ üìù T·ªëi ∆∞u h√≥a vi·ªác g·ª≠i tin nh·∫Øn d√†i

‚Ä¢ üìã Th√™m ƒë·ªãnh d·∫°ng Markdown cho ph·∫£n h·ªìi
""",
        "1.0": """
üöÄ *Phi√™n b·∫£n ban ƒë·∫ßu v1.0*

‚Ä¢ ü§ñ T√≠ch h·ª£p v·ªõi Gemini 2.0 API

‚Ä¢ üí¨ X·ª≠ l√Ω tin nh·∫Øn vƒÉn b·∫£n c∆° b·∫£n

‚Ä¢ üß† B·ªô nh·ªõ ng·∫Øn h·∫°n v√† d√†i h·∫°n

‚Ä¢ üìã L·ªánh c∆° b·∫£n: /start, /info
""",
    }

    # N·∫øu kh√¥ng c√≥ n·ªôi dung cho phi√™n b·∫£n ƒë∆∞·ª£c ch·ªçn, hi·ªÉn th·ªã th√¥ng b√°o chung
    if version not in version_content:
        content = f"*Phi√™n b·∫£n v{version}*\n\nTh√¥ng tin chi ti·∫øt v·ªÅ phi√™n b·∫£n n√†y kh√¥ng c√≥ s·∫µn."
    else:
        content = version_content[version]

    # T·∫°o n√∫t quay l·∫°i phi√™n b·∫£n m·ªõi nh·∫•t
    markup = InlineKeyboardMarkup()
    markup.add(
        InlineKeyboardButton(
            "üîô Quay l·∫°i phi√™n b·∫£n m·ªõi nh·∫•t", callback_data="version:latest"
        )
    )

    await bot.answer_callback_query(call.id)
    await bot.edit_message_text(
        chat_id=call.message.chat.id,
        message_id=call.message.message_id,
        text=content,
        parse_mode="Markdown",
        reply_markup=markup,
    )


@bot.message_handler(commands=["clear"])
async def clear_memory(message):
    user_id = message.from_user.id
    memory_handler = AdvancedMemoryHandler(user_id)
    memory_handler.clear_memory()
    await bot.reply_to(message, "‚úÖ ƒê√£ x√≥a b·ªô nh·ªõ c·ªßa b·∫°n.")


@bot.message_handler(commands=["search"])
async def handle_search(message):
    query = message.text.split("/search", 1)[-1].strip()
    if not query:
        await bot.reply_to(
            message, "‚ùó Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm sau l·ªánh /search."
        )
        return

    try:
        thinking_message = await bot.reply_to(message, "üîç ƒêang t√¨m ki·∫øm...")

        async with aiohttp.ClientSession() as session:
            search_result = await generate_search_response(
                query, message.from_user.id, session
            )

        max_length = 4096
        if len(search_result) > max_length:
            chunks = [
                search_result[i : i + max_length]
                for i in range(0, len(search_result), max_length)
            ]
            for i, chunk in enumerate(chunks):
                if i == 0:
                    sent_message = await bot.edit_message_text(
                        chat_id=thinking_message.chat.id,
                        message_id=thinking_message.message_id,
                        text=chunk,
                        parse_mode="Markdown",
                    )
                else:
                    sent_message = await bot.send_message(
                        chat_id=thinking_message.chat.id,
                        text=chunk,
                        parse_mode="Markdown",
                    )

        else:
            sent_message = await bot.edit_message_text(
                chat_id=thinking_message.chat.id,
                message_id=thinking_message.message_id,
                text=search_result,
                parse_mode="Markdown",
            )

    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω t√¨m ki·∫øm: {str(e)}")
        error_message = format_error_message(
            ErrorCodes.UNKNOWN_ERROR,
            "ƒê√£ x·∫£y ra l·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm. Vui l√≤ng th·ª≠ l·∫°i sau.",
        )
        await bot.edit_message_text(
            chat_id=thinking_message.chat.id,
            message_id=thinking_message.message_id,
            text=error_message,
            parse_mode="Markdown",
        )


async def generate_search_response(
    query: str, user_id: int, session: aiohttp.ClientSession
) -> str:
    prompt_generator = AdvancedPromptGenerator()

    search_prompt = f"""H√£y t·∫°o ra m·ªôt k·∫øt qu·∫£ t√¨m ki·∫øm gi·∫£ l·∫≠p cho truy v·∫•n: "{query}".
    K·∫øt qu·∫£ n√™n bao g·ªìm:
    1. M·ªôt t√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ ch·ªß ƒë·ªÅ (2-3 c√¢u)
    2. 3-5 ƒëi·ªÉm ch√≠nh li√™n quan ƒë·∫øn truy v·∫•n
    3. 2-3 li√™n k·∫øt gi·∫£ ƒë·ªãnh (kh√¥ng c·∫ßn ph·∫£i l√† li√™n k·∫øt th·∫≠t) ƒë·∫øn c√°c ngu·ªìn th√¥ng tin li√™n quan
    
    H√£y ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ b·∫±ng Markdown v√† s·ª≠ d·ª•ng emoji ph√π h·ª£p."""

    enhanced_prompt = prompt_generator.generate_enhanced_prompt(search_prompt, [])

    try:
        current_key = await api_key_manager.get_current_key()
        url = f"{API_URL}?key={current_key}"
        headers = {"Content-Type": "application/json"}

        data = {
            "contents": [{"parts": [{"text": enhanced_prompt}]}],
            "generationConfig": {
                "temperature": 0.7,
                "topK": 40,
                "topP": 0.95,
                "maxOutputTokens": 2048,
            },
        }

        async with session.post(
            url, headers=headers, json=data, timeout=30
        ) as response:
            if response.status == 429:
                new_key = await api_key_manager.handle_error(429)
                url = f"{API_URL}?key={new_key}"
                async with session.post(
                    url, headers=headers, json=data, timeout=30
                ) as retry_response:
                    result = await retry_response.json()
            else:
                result = await response.json()

        if "candidates" in result and result["candidates"]:
            search_result = normalize_utf8(
                result["candidates"][0]["content"]["parts"][0]["text"]
            )
            return f"üîç K·∫øt qu·∫£ t√¨m ki·∫øm cho '*{query}*':\n\n{search_result}"
        else:
            return format_error_message(
                ErrorCodes.API_ERROR, "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p."
            )

    except Exception as e:
        logger.error(f"L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm: {str(e)}")
        return format_error_message(
            ErrorCodes.UNKNOWN_ERROR, f"ƒê√£ x·∫£y ra l·ªói khi t√¨m ki·∫øm: {str(e)}"
        )


# C·∫•u h√¨nh cho image generation
IMAGE_CONFIG = {
    "models": {
        "flux": {
            "name": "flux",
            "display_name": "Flux",
            "max_retries": 3,
            "timeout": 60,
            "cooldown": 5,  # Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn retry (gi√¢y)
        },
        "dalle": {
            "name": "dall-e-3",
            "display_name": "DALL-E 3",
            "max_retries": 2,
            "timeout": 90,
            "cooldown": 10,
        },
    },
    "default_model": "flux",
    "max_concurrent_generations": 5,  # S·ªë l∆∞·ª£ng t·ªëi ƒëa c√°c y√™u c·∫ßu t·∫°o ·∫£nh ƒë·ªìng th·ªùi
    "cache_size": 100,  # S·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë∆∞·ª£c cache
    "temp_dir": "temp_images",  # Th∆∞ m·ª•c l∆∞u ·∫£nh t·∫°m th·ªùi
    "default_negative_prompt": "ugly, deformed, noisy, blurry, distorted, out of focus, bad anatomy, extra limbs, poorly drawn face, poorly drawn hands, missing fingers",
}

# T·∫°o th∆∞ m·ª•c temp n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(IMAGE_CONFIG["temp_dir"], exist_ok=True)

# --- INITIALIZATION ---
# Kh·ªüi t·∫°o g4f client
g4f_client = Client()

# Semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng y√™u c·∫ßu ƒë·ªìng th·ªùi
image_generation_semaphore = asyncio.Semaphore(
    IMAGE_CONFIG["max_concurrent_generations"]
)

# Theo d√µi c√°c y√™u c·∫ßu ƒëang x·ª≠ l√Ω
active_requests = {}


# --- UTILITY FUNCTIONS ---
@lru_cache(maxsize=IMAGE_CONFIG["cache_size"])
def get_cached_image_url(
    prompt: str, model: str, negative_prompt: str = ""
) -> Optional[str]:
    """Cache k·∫øt qu·∫£ t·∫°o ·∫£nh ƒë·ªÉ t√°i s·ª≠ d·ª•ng"""
    return None  # Ban ƒë·∫ßu cache tr·ªëng


def update_image_cache(prompt: str, model: str, negative_prompt: str, url: str) -> None:
    """C·∫≠p nh·∫≠t cache v·ªõi URL ·∫£nh m·ªõi t·∫°o"""
    get_cached_image_url.cache_clear()  # X√≥a cache c≈©
    get_cached_image_url(prompt, model, negative_prompt)  # Th√™m v√†o cache


def extract_model_from_command(text: str) -> Tuple[str, str]:
    """Tr√≠ch xu·∫•t model v√† prompt t·ª´ l·ªánh"""
    parts = text.split(" ", 2)

    if len(parts) < 2:
        return IMAGE_CONFIG["default_model"], ""

    if len(parts) == 2:
        return IMAGE_CONFIG["default_model"], parts[1].strip()

    # Ki·ªÉm tra xem ph·∫ßn th·ª© hai c√≥ ph·∫£i l√† model kh√¥ng
    potential_model = parts[1].lower().strip()
    if potential_model in IMAGE_CONFIG["models"]:
        return potential_model, parts[2].strip()
    else:
        # N·∫øu kh√¥ng ph·∫£i model, gh√©p ph·∫ßn 1 v√† 2 l√†m prompt
        return IMAGE_CONFIG["default_model"], " ".join(parts[1:]).strip()


def is_english(text: str) -> bool:
    """Ki·ªÉm tra xem vƒÉn b·∫£n c√≥ ph·∫£i ti·∫øng Anh kh√¥ng (c·∫£i ti·∫øn)"""
    # Danh s√°ch c√°c t·ª´ th√¥ng d·ª•ng trong ti·∫øng Anh
    common_english_words = {
        "the",
        "be",
        "to",
        "of",
        "and",
        "a",
        "in",
        "that",
        "have",
        "I",
        "it",
        "for",
        "not",
        "on",
        "with",
        "he",
        "as",
        "you",
        "do",
        "at",
    }

    # T√°ch vƒÉn b·∫£n th√†nh c√°c t·ª´
    words = text.lower().split()

    # N·∫øu kh√¥ng c√≥ t·ª´ n√†o, kh√¥ng th·ªÉ x√°c ƒë·ªãnh
    if not words:
        return False

    # ƒê·∫øm s·ªë t·ª´ ti·∫øng Anh th√¥ng d·ª•ng
    english_word_count = sum(1 for word in words if word in common_english_words)

    # N·∫øu c√≥ √≠t nh·∫•t 20% t·ª´ ti·∫øng Anh th√¥ng d·ª•ng, coi l√† ti·∫øng Anh
    return english_word_count / len(words) >= 0.2


async def translate_to_english(text: str, user_id: int) -> str:
    """D·ªãch vƒÉn b·∫£n sang ti·∫øng Anh s·ª≠ d·ª•ng Gemini API"""
    try:
        logger.info(f"Translating prompt to English for user {user_id}: {text}")

        # S·ª≠ d·ª•ng h√†m generate_response hi·ªán c√≥ ƒë·ªÉ d·ªãch
        translation_prompt = f"Translate the following text to English, keep it concise and suitable for image generation. Only return the translation, nothing else: {text}"

        async with aiohttp.ClientSession() as session:
            translated_text = await generate_response(
                translation_prompt, user_id, session
            )

            # L√†m s·∫°ch k·∫øt qu·∫£ d·ªãch
            translated_text = translated_text.replace("Translation:", "").strip()
            translated_text = translated_text.replace("*", "").strip()
            translated_text = translated_text.replace('"', "").strip()

            logger.info(f"Translated prompt: {translated_text}")
            return translated_text
    except Exception as e:
        logger.error(f"Translation error: {str(e)}")
        # N·∫øu d·ªãch th·∫•t b·∫°i, tr·∫£ v·ªÅ vƒÉn b·∫£n g·ªëc
        return text


async def optimize_image(image_data: bytes, quality: int = 90) -> bytes:
    """T·ªëi ∆∞u h√≥a ·∫£nh ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc m√† v·∫´n gi·ªØ ch·∫•t l∆∞·ª£ng"""
    try:
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω ·∫£nh kh√¥ng ch·∫∑n event loop
        loop = asyncio.get_event_loop()
        optimized_data = await loop.run_in_executor(
            executor, lambda: optimize_image_sync(image_data, quality)
        )
        return optimized_data
    except Exception as e:
        logger.error(f"Image optimization error: {str(e)}")
        # N·∫øu t·ªëi ∆∞u th·∫•t b·∫°i, tr·∫£ v·ªÅ ·∫£nh g·ªëc
        return image_data


def optimize_image_sync(image_data: bytes, quality: int = 90) -> bytes:
    """Phi√™n b·∫£n ƒë·ªìng b·ªô c·ªßa optimize_image ƒë·ªÉ ch·∫°y trong executor"""
    try:
        img = Image.open(io.BytesIO(image_data))
        output = io.BytesIO()

        # Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng g·ªëc n·∫øu c√≥ th·ªÉ
        if img.format == "PNG":
            img.save(output, format="PNG", optimize=True)
        else:
            # M·∫∑c ƒë·ªãnh chuy·ªÉn sang JPEG v·ªõi ch·∫•t l∆∞·ª£ng t√πy ch·ªânh
            img.convert("RGB").save(
                output, format="JPEG", quality=quality, optimize=True
            )

        return output.getvalue()
    except Exception as e:
        logger.error(f"Sync image optimization error: {str(e)}")
        return image_data


def get_progress_bar(percent: int, length: int = 10) -> str:
    """T·∫°o thanh ti·∫øn tr√¨nh d·∫°ng text"""
    filled = int(percent * length / 100)
    bar = "‚ñà" * filled + "‚ñë" * (length - filled)
    return f"{bar} {percent}%"


def get_random_wait_messages() -> str:
    """Tr·∫£ v·ªÅ th√¥ng b√°o ch·ªù ƒë·ª£i ng·∫´u nhi√™n"""
    messages = [
        "üé® ƒêang v·∫Ω t√°c ph·∫©m c·ªßa b·∫°n...",
        "‚ú® ƒêang t·∫°o h√¨nh ·∫£nh, ch·ªâ m·ªôt ch√∫t n·ªØa th√¥i...",
        "üñåÔ∏è ƒêang ph√°c h·ªça chi ti·∫øt...",
        "üîÆ ƒêang bi·∫øn √Ω t∆∞·ªüng th√†nh h√¨nh ·∫£nh...",
        "üßô‚Äç‚ôÇÔ∏è ƒêang th·ª±c hi·ªán ph√©p thu·∫≠t AI...",
        "üé≠ ƒêang s√°ng t·∫°o ngh·ªá thu·∫≠t cho b·∫°n...",
        "üåà ƒêang th√™m m√†u s·∫Øc v√†o t√°c ph·∫©m...",
        "üì∏ ƒêang x·ª≠ l√Ω h√¨nh ·∫£nh c·ªßa b·∫°n...",
        "‚ö° ƒêang t·∫°o h√¨nh ·∫£nh v·ªõi t·ªëc ƒë·ªô tia ch·ªõp...",
        "üß† AI ƒëang suy nghƒ© v·ªÅ t√°c ph·∫©m c·ªßa b·∫°n...",
    ]
    return random.choice(messages)


# --- CORE IMAGE GENERATION FUNCTIONS ---
async def generate_image_with_g4f(
    prompt: str, model: str = "flux", negative_prompt: str = ""
) -> Optional[str]:
    """T·∫°o h√¨nh ·∫£nh s·ª≠ d·ª•ng g4f client v·ªõi x·ª≠ l√Ω l·ªói v√† retry"""
    model_config = IMAGE_CONFIG["models"].get(
        model, IMAGE_CONFIG["models"][IMAGE_CONFIG["default_model"]]
    )
    model_name = model_config["name"]
    max_retries = model_config["max_retries"]
    timeout = model_config["timeout"]
    cooldown = model_config["cooldown"]

    # Ki·ªÉm tra cache tr∆∞·ªõc
    cached_url = get_cached_image_url(prompt, model, negative_prompt)
    if cached_url:
        logger.info(f"Cache hit for prompt: {prompt[:30]}...")
        return cached_url

    for attempt in range(max_retries):
        try:
            logger.info(
                f"Generating image with {model_name}, attempt {attempt+1}/{max_retries}"
            )

            # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ kh√¥ng ch·∫∑n event loop
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                executor,
                lambda: g4f_client.images.generate(
                    model=model_name,
                    prompt=prompt,
                    negative_prompt=(
                        negative_prompt
                        if negative_prompt
                        else IMAGE_CONFIG["default_negative_prompt"]
                    ),
                    response_format="url",
                ),
            )

            if (
                response
                and hasattr(response, "data")
                and response.data
                and len(response.data) > 0
            ):
                image_url = response.data[0].url

                # C·∫≠p nh·∫≠t cache
                update_image_cache(prompt, model, negative_prompt, image_url)

                logger.info(f"Successfully generated image with {model_name}")
                return image_url
            else:
                logger.warning(f"No image URL in the response from {model_name}")

                if attempt < max_retries - 1:
                    logger.info(f"Waiting {cooldown}s before retry...")
                    await asyncio.sleep(cooldown)
        except Exception as e:
            logger.error(
                f"Error in generate_image_with_g4f (attempt {attempt+1}): {str(e)}"
            )

            if attempt < max_retries - 1:
                # TƒÉng th·ªùi gian ch·ªù theo c·∫•p s·ªë nh√¢n
                wait_time = cooldown * (attempt + 1)
                logger.info(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)

    # N·∫øu t·∫•t c·∫£ c√°c l·∫ßn th·ª≠ ƒë·ªÅu th·∫•t b·∫°i
    logger.error(f"All {max_retries} attempts to generate image failed")
    return None


async def download_image(url: str, timeout: int = 30) -> Optional[bytes]:
    """T·∫£i ·∫£nh t·ª´ URL v·ªõi timeout v√† x·ª≠ l√Ω l·ªói"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    image_data = await response.read()

                    # T·ªëi ∆∞u h√≥a ·∫£nh
                    optimized_data = await optimize_image(image_data)

                    return optimized_data
                else:
                    logger.error(f"Failed to download image: HTTP {response.status}")
                    return None
    except asyncio.TimeoutError:
        logger.error(f"Timeout while downloading image from {url}")
        return None
    except Exception as e:
        logger.error(f"Error downloading image from {url}: {str(e)}")
        return None


async def save_temp_image(image_data: bytes, user_id: int) -> str:
    """L∆∞u ·∫£nh t·∫°m th·ªùi v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n"""
    # T·∫°o t√™n file duy nh·∫•t
    timestamp = int(time.time())
    filename = f"{IMAGE_CONFIG['temp_dir']}/img_{user_id}_{timestamp}.jpg"

    try:
        with open(filename, "wb") as f:
            f.write(image_data)
        return filename
    except Exception as e:
        logger.error(f"Error saving temporary image: {str(e)}")
        # S·ª≠ d·ª•ng tempfile n·∫øu c√°ch th√¥ng th∆∞·ªùng th·∫•t b·∫°i
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp:
                temp.write(image_data)
                return temp.name
        except Exception as e2:
            logger.error(f"Error using tempfile: {str(e2)}")
            return ""


# --- COMMAND HANDLERS ---
@bot.message_handler(commands=["image"])
async def handle_image_generation(message: Message):
    """X·ª≠ l√Ω l·ªánh /image ƒë·ªÉ t·∫°o h√¨nh ·∫£nh"""
    user_id = message.from_user.id
    chat_id = message.chat.id
    message_id = message.message_id

    # Tr√≠ch xu·∫•t model v√† prompt
    model, prompt = extract_model_from_command(message.text)

    if not prompt:
        usage_message = (
            "‚ö†Ô∏è *H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng l·ªánh /image*\n\n"
            "C√∫ ph√°p: `/image [model] prompt`\n\n"
            "*C√°c model h·ªó tr·ª£:*\n"
        )

        for model_key, model_info in IMAGE_CONFIG["models"].items():
            usage_message += f"‚Ä¢ `{model_key}` - {model_info['display_name']}\n"

        usage_message += "\n*V√≠ d·ª•:*\n"
        usage_message += "‚Ä¢ `/image a white siamese cat`\n"
        usage_message += "‚Ä¢ `/image flux a futuristic city at night`\n"
        usage_message += "‚Ä¢ `/image dalle a portrait of a viking warrior`\n\n"
        usage_message += "M·∫∑c ƒë·ªãnh s·∫Ω s·ª≠ d·ª•ng model `flux` n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh."

        await bot.reply_to(message, usage_message, parse_mode="Markdown")
        return

    # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ƒëang c√≥ y√™u c·∫ßu ƒëang x·ª≠ l√Ω kh√¥ng
    if user_id in active_requests:
        await bot.reply_to(
            message,
            "‚ö†Ô∏è B·∫°n ƒë√£ c√≥ m·ªôt y√™u c·∫ßu t·∫°o h√¨nh ·∫£nh ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω. Vui l√≤ng ƒë·ª£i ho√†n t·∫•t tr∆∞·ªõc khi t·∫°o ·∫£nh m·ªõi.",
            parse_mode="Markdown",
        )
        return

    # ƒê√°nh d·∫•u ng∆∞·ªùi d√πng ƒëang c√≥ y√™u c·∫ßu ƒëang x·ª≠ l√Ω
    active_requests[user_id] = {
        "prompt": prompt,
        "model": model,
        "start_time": time.time(),
    }

    # G·ª≠i th√¥ng b√°o ch·ªù ƒë·ª£i
    waiting_message = await bot.reply_to(
        message,
        f"üé® *ƒêang chu·∫©n b·ªã t·∫°o h√¨nh ·∫£nh*\n\n"
        f"‚Ä¢ *Prompt:* {prompt}\n"
        f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
        f"{get_progress_bar(5)} (ƒêang kh·ªüi t·∫°o...)",
        parse_mode="Markdown",
    )

    # C·∫≠p nh·∫≠t th√¥ng b√°o ch·ªù ƒë·ª£i theo ti·∫øn tr√¨nh
    update_task = asyncio.create_task(
        update_waiting_message(waiting_message, prompt, model)
    )

    try:
        # Ki·ªÉm tra v√† d·ªãch prompt n·∫øu kh√¥ng ph·∫£i ti·∫øng Anh
        if not is_english(prompt):
            # C·∫≠p nh·∫≠t th√¥ng b√°o
            await bot.edit_message_text(
                f"üé® *ƒêang chu·∫©n b·ªã t·∫°o h√¨nh ·∫£nh*\n\n"
                f"‚Ä¢ *Prompt:* {prompt}\n"
                f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                f"{get_progress_bar(10)} (ƒêang d·ªãch prompt...)",
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                parse_mode="Markdown",
            )

            # D·ªãch prompt sang ti·∫øng Anh
            translated_prompt = await translate_to_english(prompt, user_id)
            logger.info(f"Translated prompt from '{prompt}' to '{translated_prompt}'")

            # C·∫≠p nh·∫≠t th√¥ng b√°o v·ªõi prompt ƒë√£ d·ªãch
            await bot.edit_message_text(
                f"üé® *ƒêang chu·∫©n b·ªã t·∫°o h√¨nh ·∫£nh*\n\n"
                f"‚Ä¢ *Prompt g·ªëc:* {prompt}\n"
                f"‚Ä¢ *Prompt ƒë√£ d·ªãch:* {translated_prompt}\n"
                f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                f"{get_progress_bar(20)} (ƒêang t·∫°o h√¨nh ·∫£nh...)",
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                parse_mode="Markdown",
            )

            # S·ª≠ d·ª•ng prompt ƒë√£ d·ªãch
            prompt = translated_prompt

        # S·ª≠ d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng y√™u c·∫ßu ƒë·ªìng th·ªùi
        async with image_generation_semaphore:
            # C·∫≠p nh·∫≠t th√¥ng b√°o
            await bot.edit_message_text(
                f"üé® *ƒêang t·∫°o h√¨nh ·∫£nh*\n\n"
                f"‚Ä¢ *Prompt:* {prompt}\n"
                f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                f"{get_progress_bar(30)} (ƒêang x·ª≠ l√Ω...)",
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                parse_mode="Markdown",
            )

            # T·∫°o h√¨nh ·∫£nh
            start_time = time.time()
            image_url = await generate_image_with_g4f(prompt, model)

            if not image_url:
                # Th·ª≠ l·∫°i v·ªõi model kh√°c n·∫øu model hi·ªán t·∫°i th·∫•t b·∫°i
                fallback_models = [
                    m for m in IMAGE_CONFIG["models"].keys() if m != model
                ]

                if fallback_models:
                    fallback_model = fallback_models[0]
                    logger.info(
                        f"Trying fallback model {fallback_model} after {model} failed"
                    )

                    await bot.edit_message_text(
                        f"üé® *ƒêang t·∫°o h√¨nh ·∫£nh*\n\n"
                        f"‚Ä¢ *Prompt:* {prompt}\n"
                        f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']} (th·∫•t b·∫°i, ƒëang th·ª≠ v·ªõi {IMAGE_CONFIG['models'][fallback_model]['display_name']})\n\n"
                        f"{get_progress_bar(40)} (ƒêang th·ª≠ l·∫°i...)",
                        chat_id=waiting_message.chat.id,
                        message_id=waiting_message.message_id,
                        parse_mode="Markdown",
                    )

                    image_url = await generate_image_with_g4f(prompt, fallback_model)

            # H·ªßy task c·∫≠p nh·∫≠t th√¥ng b√°o ch·ªù ƒë·ª£i
            update_task.cancel()

            if image_url:
                # C·∫≠p nh·∫≠t th√¥ng b√°o
                await bot.edit_message_text(
                    f"üé® *ƒêang t·∫°o h√¨nh ·∫£nh*\n\n"
                    f"‚Ä¢ *Prompt:* {prompt}\n"
                    f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                    f"{get_progress_bar(70)} (ƒêang t·∫£i h√¨nh ·∫£nh...)",
                    chat_id=waiting_message.chat.id,
                    message_id=waiting_message.message_id,
                    parse_mode="Markdown",
                )

                # T·∫£i h√¨nh ·∫£nh
                image_data = await download_image(image_url)

                if image_data:
                    # L∆∞u ·∫£nh t·∫°m th·ªùi
                    temp_image_path = await save_temp_image(image_data, user_id)

                    if temp_image_path:
                        # T√≠nh th·ªùi gian x·ª≠ l√Ω
                        end_time = time.time()
                        processing_time = end_time - start_time

                        # C·∫≠p nh·∫≠t th√¥ng b√°o
                        await bot.edit_message_text(
                            f"üé® *ƒêang t·∫°o h√¨nh ·∫£nh*\n\n"
                            f"‚Ä¢ *Prompt:* {prompt}\n"
                            f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                            f"{get_progress_bar(90)} (ƒêang g·ª≠i h√¨nh ·∫£nh...)",
                            chat_id=waiting_message.chat.id,
                            message_id=waiting_message.message_id,
                            parse_mode="Markdown",
                        )

                        # G·ª≠i h√¨nh ·∫£nh
                        with open(temp_image_path, "rb") as photo:
                            caption = (
                                f'üñºÔ∏è *H√¨nh ·∫£nh t·ª´ prompt*: "{prompt}"\n'
                                f"ü§ñ *Model*: {IMAGE_CONFIG['models'][model]['display_name']}\n"
                                f"‚è±Ô∏è *Th·ªùi gian x·ª≠ l√Ω*: {processing_time:.2f}s"
                            )

                            sent_message = await bot.send_photo(
                                chat_id,
                                photo,
                                caption=caption,
                                reply_to_message_id=message_id,
                                parse_mode="Markdown",
                            )

                            # Th√™m c√°c n√∫t t∆∞∆°ng t√°c
                            markup = InlineKeyboardMarkup(row_width=2)
                            markup.add(
                                InlineKeyboardButton(
                                    "üîÑ T·∫°o l·∫°i",
                                    callback_data=truncate_callback_data(
                                        "regenerate_image", prompt
                                    ),
                                ),
                                InlineKeyboardButton(
                                    "üîÄ Bi·∫øn th·ªÉ",
                                    callback_data=truncate_callback_data(
                                        "variant_image", prompt
                                    ),
                                ),
                            )

                            # Th√™m n√∫t ch·ªçn model kh√°c
                            model_buttons = []
                            for m_key, m_info in IMAGE_CONFIG["models"].items():
                                if m_key != model:  # Ch·ªâ hi·ªÉn th·ªã c√°c model kh√°c
                                    model_buttons.append(
                                        InlineKeyboardButton(
                                            f"üîÑ Th·ª≠ v·ªõi {m_info['display_name']}",
                                            callback_data=f"change_model:{m_key}:{truncate_callback_data('prompt', prompt, 30)}",
                                        )
                                    )

                            # Th√™m c√°c n√∫t model theo t·ª´ng h√†ng
                            for button in model_buttons:
                                markup.add(button)

                            await bot.edit_message_reply_markup(
                                chat_id=sent_message.chat.id,
                                message_id=sent_message.message_id,
                                reply_markup=markup,
                            )

                        # X√≥a th√¥ng b√°o ch·ªù ƒë·ª£i
                        await bot.delete_message(
                            chat_id=waiting_message.chat.id,
                            message_id=waiting_message.message_id,
                        )

                        # X√≥a file t·∫°m
                        try:
                            os.remove(temp_image_path)
                        except:
                            pass
                    else:
                        raise Exception("Kh√¥ng th·ªÉ l∆∞u h√¨nh ·∫£nh t·∫°m th·ªùi")
                else:
                    raise Exception("Kh√¥ng th·ªÉ t·∫£i h√¨nh ·∫£nh t·ª´ URL")
            else:
                raise Exception("Kh√¥ng th·ªÉ t·∫°o h√¨nh ·∫£nh v·ªõi t·∫•t c·∫£ c√°c model")

    except asyncio.CancelledError:
        logger.warning(f"Image generation for user {user_id} was cancelled")
        try:
            await bot.edit_message_text(
                "‚ùå *Y√™u c·∫ßu t·∫°o h√¨nh ·∫£nh ƒë√£ b·ªã h·ªßy*",
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                parse_mode="Markdown",
            )
        except:
            pass

    except Exception as e:
        logger.error(f"Error in handle_image_generation: {str(e)}")
        logger.error(traceback.format_exc())

        error_message = format_error_message(
            ErrorCodes.UNKNOWN_ERROR, f"ƒê√£ x·∫£y ra l·ªói khi t·∫°o h√¨nh ·∫£nh: {str(e)}"
        )

        try:
            await bot.edit_message_text(
                error_message,
                chat_id=waiting_message.chat.id,
                message_id=waiting_message.message_id,
                parse_mode="Markdown",
            )
        except:
            try:
                await bot.send_message(
                    chat_id,
                    error_message,
                    reply_to_message_id=message_id,
                    parse_mode="Markdown",
                )
            except:
                pass

    finally:
        # X√≥a ng∆∞·ªùi d√πng kh·ªèi danh s√°ch y√™u c·∫ßu ƒëang x·ª≠ l√Ω
        if user_id in active_requests:
            del active_requests[user_id]

        # ƒê·∫£m b·∫£o task c·∫≠p nh·∫≠t th√¥ng b√°o ch·ªù ƒë·ª£i ƒë√£ b·ªã h·ªßy
        if update_task and not update_task.done():
            update_task.cancel()


async def update_waiting_message(message: Message, prompt: str, model: str):
    """C·∫≠p nh·∫≠t th√¥ng b√°o ch·ªù ƒë·ª£i v·ªõi th√¥ng tin ti·∫øn tr√¨nh"""
    try:
        progress = 10
        while True:
            if progress >= 60:  # Gi·ªõi h·∫°n ti·∫øn tr√¨nh gi·∫£ ·ªü 60%
                progress = 10  # Reset l·∫°i ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng ch·ªù ƒë·ª£i

            wait_message = get_random_wait_messages()

            await bot.edit_message_text(
                f"üé® *ƒêang t·∫°o h√¨nh ·∫£nh*\n\n"
                f"‚Ä¢ *Prompt:* {prompt}\n"
                f"‚Ä¢ *Model:* {IMAGE_CONFIG['models'][model]['display_name']}\n\n"
                f"{get_progress_bar(progress)} ({wait_message})",
                chat_id=message.chat.id,
                message_id=message.message_id,
                parse_mode="Markdown",
            )

            progress += 5
            await asyncio.sleep(3)  # C·∫≠p nh·∫≠t m·ªói 3 gi√¢y

    except asyncio.CancelledError:
        # Task b·ªã h·ªßy, kh√¥ng c·∫ßn l√†m g√¨
        pass
    except Exception as e:
        logger.error(f"Error updating waiting message: {str(e)}")


# --- HELP COMMAND ---
@bot.message_handler(commands=["imagehelp"])
async def handle_image_help(message: Message):
    """Hi·ªÉn th·ªã tr·ª£ gi√∫p v·ªÅ l·ªánh /image"""
    help_text = (
        "üñºÔ∏è *H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng l·ªánh /image*\n\n"
        "*C√∫ ph√°p c∆° b·∫£n:*\n"
        "`/image [model] prompt`\n\n"
        "*C√°c model h·ªó tr·ª£:*\n"
    )

    for model_key, model_info in IMAGE_CONFIG["models"].items():
        help_text += f"‚Ä¢ `{model_key}` - {model_info['display_name']}\n"

    help_text += (
        "\n*V√≠ d·ª•:*\n"
        "‚Ä¢ `/image a white siamese cat`\n"
        "‚Ä¢ `/image flux a futuristic city at night`\n"
        "‚Ä¢ `/image dalle a portrait of a viking warrior`\n\n"
        "*L∆∞u √Ω:*\n"
        "‚Ä¢ N√™n s·ª≠ d·ª•ng ti·∫øng Anh ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t\n"
        "‚Ä¢ N·∫øu s·ª≠ d·ª•ng ti·∫øng Vi·ªát, bot s·∫Ω t·ª± ƒë·ªông d·ªãch sang ti·∫øng Anh\n"
        "‚Ä¢ M·ªói ng∆∞·ªùi d√πng ch·ªâ c√≥ th·ªÉ t·∫°o m·ªôt h√¨nh ·∫£nh t·∫°i m·ªôt th·ªùi ƒëi·ªÉm\n"
        "‚Ä¢ Sau khi t·∫°o h√¨nh ·∫£nh, b·∫°n c√≥ th·ªÉ:\n"
        "  - T·∫°o l·∫°i h√¨nh ·∫£nh v·ªõi c√πng prompt\n"
        "  - T·∫°o bi·∫øn th·ªÉ c·ªßa h√¨nh ·∫£nh\n"
        "  - Th·ª≠ v·ªõi model kh√°c\n\n"
        "*M·∫πo t·∫°o prompt hi·ªáu qu·∫£:*\n"
        "‚Ä¢ M√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ b·∫°n mu·ªën th·∫•y\n"
        "‚Ä¢ Ch·ªâ ƒë·ªãnh phong c√°ch ngh·ªá thu·∫≠t (v√≠ d·ª•: oil painting, digital art, etc.)\n"
        "‚Ä¢ ƒê·ªÅ c·∫≠p ƒë·∫øn √°nh s√°ng, m√†u s·∫Øc, g√≥c nh√¨n n·∫øu c·∫ßn\n"
        "‚Ä¢ S·ª≠ d·ª•ng c√°c t·ª´ kh√≥a nh∆∞ 'high quality', 'detailed', 'realistic' ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng\n"
    )

    await bot.reply_to(message, help_text, parse_mode="Markdown")


# --- ADMIN COMMANDS ---
@bot.message_handler(commands=["imagestats"])
async def handle_image_stats(message: Message):
    """Hi·ªÉn th·ªã th·ªëng k√™ v·ªÅ vi·ªác s·ª≠ d·ª•ng l·ªánh /image (ch·ªâ d√†nh cho admin)"""
    # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ph·∫£i l√† admin kh√¥ng
    if message.from_user.id not in [6337636891]:  # Thay YOUR_ADMIN_ID b·∫±ng ID c·ªßa b·∫°n
        await bot.reply_to(message, "‚ö†Ô∏è B·∫°n kh√¥ng c√≥ quy·ªÅn s·ª≠ d·ª•ng l·ªánh n√†y.")
        return

    # T√≠nh to√°n th·ªëng k√™
    stats = {
        "active_requests": len(active_requests),
        "cache_info": get_cached_image_url.cache_info(),
        "semaphore_value": image_generation_semaphore._value,  # S·ªë slot c√≤n tr·ªëng
    }

    stats_text = (
        "üìä *Th·ªëng k√™ l·ªánh /image*\n\n"
        f"‚Ä¢ *Y√™u c·∫ßu ƒëang x·ª≠ l√Ω:* {stats['active_requests']}/{IMAGE_CONFIG['max_concurrent_generations']}\n"
        f"‚Ä¢ *Cache:* {stats['cache_info'].hits} hits, {stats['cache_info'].misses} misses\n"
        f"‚Ä¢ *T·ª∑ l·ªá cache hit:* {stats['cache_info'].hits/(stats['cache_info'].hits+stats['cache_info'].misses)*100:.1f}% (n·∫øu c√≥)\n"
        f"‚Ä¢ *Slot x·ª≠ l√Ω c√≤n tr·ªëng:* {stats['semaphore_value']}/{IMAGE_CONFIG['max_concurrent_generations']}\n"
    )

    if active_requests:
        stats_text += "\n*Y√™u c·∫ßu ƒëang x·ª≠ l√Ω:*\n"
        for user_id, request in active_requests.items():
            elapsed = time.time() - request["start_time"]
            stats_text += f"‚Ä¢ User {user_id}: {request['model']} - '{request['prompt'][:20]}...' ({elapsed:.1f}s)\n"

    await bot.reply_to(message, stats_text, parse_mode="Markdown")


@bot.message_handler(commands=["imageclear"])
async def handle_image_clear(message: Message):
    """X√≥a cache v√† reset tr·∫°ng th√°i (ch·ªâ d√†nh cho admin)"""
    # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ph·∫£i l√† admin kh√¥ng
    if message.from_user.id not in [6337636891]:  # Thay YOUR_ADMIN_ID b·∫±ng ID c·ªßa b·∫°n
        await bot.reply_to(message, "‚ö†Ô∏è B·∫°n kh√¥ng c√≥ quy·ªÅn s·ª≠ d·ª•ng l·ªánh n√†y.")
        return

    # X√≥a cache
    get_cached_image_url.cache_clear()

    # Reset active_requests (c·∫©n th·∫≠n v·ªõi ƒëi·ªÅu n√†y)
    active_requests.clear()

    # X√≥a c√°c file t·∫°m
    try:
        for file in os.listdir(IMAGE_CONFIG["temp_dir"]):
            file_path = os.path.join(IMAGE_CONFIG["temp_dir"], file)
            if os.path.isfile(file_path):
                os.remove(file_path)
    except Exception as e:
        logger.error(f"Error clearing temp files: {str(e)}")

    await bot.reply_to(
        message,
        "‚úÖ ƒê√£ x√≥a cache v√† reset tr·∫°ng th√°i th√†nh c√¥ng.",
        parse_mode="Markdown",
    )


async def main():
    logger.info(
        "Bot ƒëang ch·∫°y v·ªõi m√¥ h√¨nh Gemini 2.0, b·ªô nh·ªõ t·∫°m th·ªùi v√† kh·∫£ nƒÉng ph√¢n t√≠ch h√¨nh ·∫£nh..."
    )
    while True:
        try:
            await bot.polling(
                non_stop=True, timeout=60, allowed_updates=["message", "callback_query"]
            )
        except Exception as e:
            logger.error(f"L·ªói polling: {str(e)}")
            await asyncio.sleep(10)


if __name__ == "__main__":
    asyncio.run(main())
